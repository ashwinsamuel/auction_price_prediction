0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
Starting for w300_False with mul=10
300: 50m0sec done
300: 50m10sec done
300: 50m20sec done
300: 50m30sec done
300: 50m40sec done
300: 50m50sec done
300: 51m0sec done
300: 51m10sec done
300: 51m20sec done
300: 51m30sec done
300: 51m40sec done
300: 51m50sec done
300: 52m0sec done
300: 52m10sec done
300: 52m20sec done
300: 52m30sec done
300: 52m40sec done
300: 52m50sec done
300: 53m0sec done
300: 53m10sec done
300: 53m20sec done
300: 53m30sec done
300: 53m40sec done
300: 53m50sec done
300: 54m0sec done
300: 54m10sec done
300: 54m20sec done
300: 54m30sec done
300: 54m40sec done
300: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.025860 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 47192
[LightGBM] [Info] Number of data points in the train set: 260400, number of used features: 189
[LightGBM] [Info] Start training from score 71.895004
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.538	test's l1: 61.5427
[20]	train's l1: 40.1384	test's l1: 40.2473
[30]	train's l1: 26.8224	test's l1: 26.9706
[40]	train's l1: 19.2928	test's l1: 19.7006
[50]	train's l1: 11.7746	test's l1: 12.1493
[60]	train's l1: 9.1846	test's l1: 9.89696
[70]	train's l1: 7.25731	test's l1: 8.23619
[80]	train's l1: 6.56382	test's l1: 7.62975
[90]	train's l1: 5.37207	test's l1: 6.40003
[100]	train's l1: 4.8283	test's l1: 5.91075
[110]	train's l1: 4.76687	test's l1: 5.83323
[120]	train's l1: 4.46139	test's l1: 5.58422
[130]	train's l1: 4.39133	test's l1: 5.50245
[140]	train's l1: 4.38661	test's l1: 5.5028
[150]	train's l1: 4.32103	test's l1: 5.49512
[160]	train's l1: 4.30567	test's l1: 5.49513
[170]	train's l1: 4.29926	test's l1: 5.49664
[180]	train's l1: 4.2974	test's l1: 5.49411
[190]	train's l1: 4.20678	test's l1: 5.37506
[200]	train's l1: 4.16781	test's l1: 5.3469
[210]	train's l1: 4.14025	test's l1: 5.31566
[220]	train's l1: 4.12876	test's l1: 5.30421
[230]	train's l1: 4.11888	test's l1: 5.29669
[240]	train's l1: 4.11608	test's l1: 5.29566
[250]	train's l1: 4.03896	test's l1: 5.24497
[260]	train's l1: 3.91245	test's l1: 5.14919
[270]	train's l1: 3.83782	test's l1: 5.06715
[280]	train's l1: 3.81268	test's l1: 5.05514
[290]	train's l1: 3.75957	test's l1: 5.00666
[300]	train's l1: 3.7542	test's l1: 5.00226
[310]	train's l1: 3.74814	test's l1: 4.99569
[320]	train's l1: 3.74448	test's l1: 4.99191
[330]	train's l1: 3.7216	test's l1: 4.97328
[340]	train's l1: 3.68616	test's l1: 4.93699
[350]	train's l1: 3.67327	test's l1: 4.94535
[360]	train's l1: 3.66849	test's l1: 4.95021
[370]	train's l1: 3.64798	test's l1: 4.92981
[380]	train's l1: 3.64157	test's l1: 4.92799
[390]	train's l1: 3.63875	test's l1: 4.9263
[400]	train's l1: 3.63694	test's l1: 4.92752
[410]	train's l1: 3.61031	test's l1: 4.91326
[420]	train's l1: 3.53785	test's l1: 4.87303
[430]	train's l1: 3.49201	test's l1: 4.83085
[440]	train's l1: 3.48961	test's l1: 4.82918
[450]	train's l1: 3.48495	test's l1: 4.82795
[460]	train's l1: 3.47979	test's l1: 4.81208
[470]	train's l1: 3.47037	test's l1: 4.80733
[480]	train's l1: 3.46464	test's l1: 4.80579
[490]	train's l1: 3.44756	test's l1: 4.78608
[500]	train's l1: 3.43744	test's l1: 4.77642
[510]	train's l1: 3.43443	test's l1: 4.77436
[520]	train's l1: 3.43339	test's l1: 4.77439
[530]	train's l1: 3.41862	test's l1: 4.76621
[540]	train's l1: 3.41589	test's l1: 4.76774
[550]	train's l1: 3.41138	test's l1: 4.76892
[560]	train's l1: 3.40848	test's l1: 4.76615
[570]	train's l1: 3.40741	test's l1: 4.7657
[580]	train's l1: 3.40441	test's l1: 4.76689
[590]	train's l1: 3.40202	test's l1: 4.76563
[600]	train's l1: 3.39819	test's l1: 4.7631
[610]	train's l1: 3.38074	test's l1: 4.76114
[620]	train's l1: 3.37984	test's l1: 4.76073
[630]	train's l1: 3.36648	test's l1: 4.7568
[640]	train's l1: 3.35957	test's l1: 4.75071
[650]	train's l1: 3.35715	test's l1: 4.74951
[660]	train's l1: 3.33154	test's l1: 4.72811
[670]	train's l1: 3.32463	test's l1: 4.7203
[680]	train's l1: 3.32375	test's l1: 4.72014
[690]	train's l1: 3.32084	test's l1: 4.71119
[700]	train's l1: 3.3181	test's l1: 4.70977
[710]	train's l1: 3.30283	test's l1: 4.70533
[720]	train's l1: 3.29929	test's l1: 4.70226
[730]	train's l1: 3.28811	test's l1: 4.69909
[740]	train's l1: 3.24009	test's l1: 4.685
[750]	train's l1: 3.23947	test's l1: 4.68482
[760]	train's l1: 3.21555	test's l1: 4.67618
[770]	train's l1: 3.21199	test's l1: 4.67412
[780]	train's l1: 3.21036	test's l1: 4.67424
[790]	train's l1: 3.191	test's l1: 4.66643
[800]	train's l1: 3.05846	test's l1: 4.5503
[810]	train's l1: 3.03519	test's l1: 4.54215
[820]	train's l1: 3.01726	test's l1: 4.53316
[830]	train's l1: 3.01252	test's l1: 4.53047
[840]	train's l1: 3.01052	test's l1: 4.53048
[850]	train's l1: 3.00507	test's l1: 4.52565
[860]	train's l1: 2.98882	test's l1: 4.52194
[870]	train's l1: 2.98475	test's l1: 4.52306
[880]	train's l1: 2.97054	test's l1: 4.52907
[890]	train's l1: 2.96779	test's l1: 4.52886
[900]	train's l1: 2.96492	test's l1: 4.52679
[910]	train's l1: 2.96056	test's l1: 4.52656
[920]	train's l1: 2.95666	test's l1: 4.52422
[930]	train's l1: 2.95403	test's l1: 4.52157
[940]	train's l1: 2.95257	test's l1: 4.5216
[950]	train's l1: 2.94942	test's l1: 4.52171
[960]	train's l1: 2.94783	test's l1: 4.52127
[970]	train's l1: 2.94475	test's l1: 4.52145
[980]	train's l1: 2.94139	test's l1: 4.51963
[990]	train's l1: 2.87124	test's l1: 4.48338
[1000]	train's l1: 2.86988	test's l1: 4.48303
Did not meet early stopping. Best iteration is:
[999]	train's l1: 2.86999	test's l1: 4.48297
Starting for w250_False with mul=10
250: 50m50sec done
250: 51m0sec done
250: 51m10sec done
250: 51m20sec done
250: 51m30sec done
250: 51m40sec done
250: 51m50sec done
250: 52m0sec done
250: 52m10sec done
250: 52m20sec done
250: 52m30sec done
250: 52m40sec done
250: 52m50sec done
250: 53m0sec done
250: 53m10sec done
250: 53m20sec done
250: 53m30sec done
250: 53m40sec done
250: 53m50sec done
250: 54m0sec done
250: 54m10sec done
250: 54m20sec done
250: 54m30sec done
250: 54m40sec done
250: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079085 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 51835
[LightGBM] [Info] Number of data points in the train set: 680400, number of used features: 209
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4268	test's l1: 61.3449
[20]	train's l1: 40.0325	test's l1: 40.0212
[30]	train's l1: 26.5927	test's l1: 26.6207
[40]	train's l1: 18.3886	test's l1: 18.7023
[50]	train's l1: 11.2869	test's l1: 11.5916
[60]	train's l1: 7.63411	test's l1: 8.11963
[70]	train's l1: 6.71939	test's l1: 7.43169
[80]	train's l1: 6.07009	test's l1: 7.08013
[90]	train's l1: 5.77227	test's l1: 6.88528
[100]	train's l1: 4.87301	test's l1: 6.06992
[110]	train's l1: 4.7043	test's l1: 5.87817
[120]	train's l1: 4.539	test's l1: 5.75746
[130]	train's l1: 4.31258	test's l1: 5.61111
[140]	train's l1: 4.13415	test's l1: 5.53276
[150]	train's l1: 4.02215	test's l1: 5.45711
[160]	train's l1: 3.96392	test's l1: 5.39231
[170]	train's l1: 3.88206	test's l1: 5.30169
[180]	train's l1: 3.87366	test's l1: 5.29742
[190]	train's l1: 3.86811	test's l1: 5.29475
[200]	train's l1: 3.74697	test's l1: 5.18613
[210]	train's l1: 3.73533	test's l1: 5.17226
[220]	train's l1: 3.73119	test's l1: 5.17041
[230]	train's l1: 3.71871	test's l1: 5.1582
[240]	train's l1: 3.60328	test's l1: 5.05209
[250]	train's l1: 3.58813	test's l1: 5.03938
[260]	train's l1: 3.58741	test's l1: 5.03942
[270]	train's l1: 3.55874	test's l1: 5.01561
[280]	train's l1: 3.55395	test's l1: 5.01482
[290]	train's l1: 3.51947	test's l1: 4.98503
[300]	train's l1: 3.51812	test's l1: 4.98969
[310]	train's l1: 3.5173	test's l1: 4.98969
[320]	train's l1: 3.5121	test's l1: 4.98463
[330]	train's l1: 3.50539	test's l1: 4.98262
[340]	train's l1: 3.50134	test's l1: 4.98163
[350]	train's l1: 3.4824	test's l1: 4.97296
[360]	train's l1: 3.47802	test's l1: 4.96809
[370]	train's l1: 3.47342	test's l1: 4.96593
[380]	train's l1: 3.47232	test's l1: 4.96627
[390]	train's l1: 3.38046	test's l1: 4.85909
[400]	train's l1: 3.3767	test's l1: 4.85674
[410]	train's l1: 3.37399	test's l1: 4.85584
[420]	train's l1: 3.37279	test's l1: 4.85475
[430]	train's l1: 3.35705	test's l1: 4.84722
[440]	train's l1: 3.31007	test's l1: 4.79691
[450]	train's l1: 3.30787	test's l1: 4.79542
[460]	train's l1: 3.28349	test's l1: 4.79628
[470]	train's l1: 3.27435	test's l1: 4.78999
[480]	train's l1: 3.23319	test's l1: 4.78911
[490]	train's l1: 3.13645	test's l1: 4.69935
[500]	train's l1: 3.11721	test's l1: 4.68693
[510]	train's l1: 3.11562	test's l1: 4.68587
[520]	train's l1: 3.11116	test's l1: 4.68262
[530]	train's l1: 3.10303	test's l1: 4.67642
[540]	train's l1: 3.0999	test's l1: 4.67464
[550]	train's l1: 3.09739	test's l1: 4.67366
[560]	train's l1: 3.0806	test's l1: 4.66657
[570]	train's l1: 3.06675	test's l1: 4.66506
[580]	train's l1: 3.06329	test's l1: 4.65871
[590]	train's l1: 2.92649	test's l1: 4.60575
[600]	train's l1: 2.75407	test's l1: 4.46039
[610]	train's l1: 2.66247	test's l1: 4.37102
[620]	train's l1: 2.63798	test's l1: 4.36052
[630]	train's l1: 2.63506	test's l1: 4.35867
[640]	train's l1: 2.55233	test's l1: 4.31215
[650]	train's l1: 2.5503	test's l1: 4.31248
[660]	train's l1: 2.49166	test's l1: 4.27295
[670]	train's l1: 2.48974	test's l1: 4.26976
[680]	train's l1: 2.4895	test's l1: 4.26954
[690]	train's l1: 2.48795	test's l1: 4.26997
[700]	train's l1: 2.48712	test's l1: 4.26979
[710]	train's l1: 2.48611	test's l1: 4.26945
[720]	train's l1: 2.48515	test's l1: 4.26927
[730]	train's l1: 2.48366	test's l1: 4.26899
[740]	train's l1: 2.48093	test's l1: 4.26654
[750]	train's l1: 2.47949	test's l1: 4.26545
[760]	train's l1: 2.47733	test's l1: 4.26322
[770]	train's l1: 2.47434	test's l1: 4.26476
[780]	train's l1: 2.47388	test's l1: 4.26457
[790]	train's l1: 2.46927	test's l1: 4.26161
[800]	train's l1: 2.4682	test's l1: 4.26167
[810]	train's l1: 2.45943	test's l1: 4.26
[820]	train's l1: 2.4587	test's l1: 4.25965
[830]	train's l1: 2.45828	test's l1: 4.25957
[840]	train's l1: 2.45664	test's l1: 4.25913
[850]	train's l1: 2.45618	test's l1: 4.25918
[860]	train's l1: 2.44187	test's l1: 4.24833
[870]	train's l1: 2.38356	test's l1: 4.18734
[880]	train's l1: 2.38071	test's l1: 4.18693
[890]	train's l1: 2.37834	test's l1: 4.18582
[900]	train's l1: 2.37519	test's l1: 4.18373
[910]	train's l1: 2.3394	test's l1: 4.15297
[920]	train's l1: 2.33297	test's l1: 4.14829
[930]	train's l1: 2.32617	test's l1: 4.14538
[940]	train's l1: 2.32514	test's l1: 4.145
[950]	train's l1: 2.32442	test's l1: 4.1451
[960]	train's l1: 2.32331	test's l1: 4.14521
[970]	train's l1: 2.32041	test's l1: 4.14378
[980]	train's l1: 2.31969	test's l1: 4.14346
[990]	train's l1: 2.31898	test's l1: 4.14317
[1000]	train's l1: 2.31722	test's l1: 4.14248
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.31722	test's l1: 4.14248
Starting for w200_False with mul=10
200: 51m40sec done
200: 51m50sec done
200: 52m0sec done
200: 52m10sec done
200: 52m20sec done
200: 52m30sec done
200: 52m40sec done
200: 52m50sec done
200: 53m0sec done
200: 53m10sec done
200: 53m20sec done
200: 53m30sec done
200: 53m40sec done
200: 53m50sec done
200: 54m0sec done
200: 54m10sec done
200: 54m20sec done
200: 54m30sec done
200: 54m40sec done
200: 54m50sec done
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.431113 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 56187
[LightGBM] [Info] Number of data points in the train set: 1100400, number of used features: 228
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4823	test's l1: 61.4529
[20]	train's l1: 39.8701	test's l1: 39.9284
[30]	train's l1: 26.5063	test's l1: 26.5932
[40]	train's l1: 18.3583	test's l1: 18.6943
[50]	train's l1: 11.2091	test's l1: 11.5011
[60]	train's l1: 7.35281	test's l1: 7.93161
[70]	train's l1: 5.95347	test's l1: 6.63448
[80]	train's l1: 4.99076	test's l1: 5.84121
[90]	train's l1: 4.67179	test's l1: 5.53194
[100]	train's l1: 4.49596	test's l1: 5.36852
[110]	train's l1: 4.27173	test's l1: 5.0803
[120]	train's l1: 4.26581	test's l1: 5.08171
[130]	train's l1: 4.20919	test's l1: 5.06638
[140]	train's l1: 4.19615	test's l1: 5.05324
[150]	train's l1: 4.1583	test's l1: 5.03486
[160]	train's l1: 3.87738	test's l1: 4.93477
[170]	train's l1: 3.81597	test's l1: 4.88888
[180]	train's l1: 3.79173	test's l1: 4.86856
[190]	train's l1: 3.75225	test's l1: 4.8345
[200]	train's l1: 3.69745	test's l1: 4.84398
[210]	train's l1: 3.69624	test's l1: 4.84457
[220]	train's l1: 3.69398	test's l1: 4.84416
[230]	train's l1: 3.68812	test's l1: 4.83736
[240]	train's l1: 3.67033	test's l1: 4.82494
[250]	train's l1: 3.62995	test's l1: 4.79431
[260]	train's l1: 3.60032	test's l1: 4.78837
[270]	train's l1: 3.51898	test's l1: 4.72691
[280]	train's l1: 3.49302	test's l1: 4.7219
[290]	train's l1: 3.4847	test's l1: 4.71778
[300]	train's l1: 3.47777	test's l1: 4.7109
[310]	train's l1: 3.46076	test's l1: 4.72078
[320]	train's l1: 3.43325	test's l1: 4.69538
[330]	train's l1: 3.39324	test's l1: 4.66231
[340]	train's l1: 3.33773	test's l1: 4.61291
[350]	train's l1: 3.32509	test's l1: 4.60792
[360]	train's l1: 3.27688	test's l1: 4.5792
[370]	train's l1: 3.249	test's l1: 4.56003
[380]	train's l1: 3.20263	test's l1: 4.52501
[390]	train's l1: 3.12596	test's l1: 4.50583
[400]	train's l1: 3.1251	test's l1: 4.5058
[410]	train's l1: 3.12174	test's l1: 4.50147
[420]	train's l1: 3.10903	test's l1: 4.48696
[430]	train's l1: 3.09943	test's l1: 4.47779
[440]	train's l1: 3.07247	test's l1: 4.45882
[450]	train's l1: 3.05464	test's l1: 4.44257
[460]	train's l1: 3.05187	test's l1: 4.44049
[470]	train's l1: 3.04192	test's l1: 4.43518
[480]	train's l1: 3.0361	test's l1: 4.42974
[490]	train's l1: 3.01092	test's l1: 4.42139
[500]	train's l1: 2.98151	test's l1: 4.39773
[510]	train's l1: 2.98105	test's l1: 4.39767
[520]	train's l1: 2.96841	test's l1: 4.39883
[530]	train's l1: 2.96616	test's l1: 4.39739
[540]	train's l1: 2.9601	test's l1: 4.38785
[550]	train's l1: 2.95789	test's l1: 4.38717
[560]	train's l1: 2.86781	test's l1: 4.32644
[570]	train's l1: 2.86708	test's l1: 4.32634
[580]	train's l1: 2.84904	test's l1: 4.30864
[590]	train's l1: 2.8477	test's l1: 4.30939
[600]	train's l1: 2.84654	test's l1: 4.30872
[610]	train's l1: 2.84513	test's l1: 4.3074
[620]	train's l1: 2.83296	test's l1: 4.3037
[630]	train's l1: 2.82453	test's l1: 4.29221
[640]	train's l1: 2.80375	test's l1: 4.27769
[650]	train's l1: 2.79823	test's l1: 4.27421
[660]	train's l1: 2.79713	test's l1: 4.27389
[670]	train's l1: 2.79304	test's l1: 4.27366
[680]	train's l1: 2.78953	test's l1: 4.27167
[690]	train's l1: 2.78163	test's l1: 4.26765
[700]	train's l1: 2.7808	test's l1: 4.26739
[710]	train's l1: 2.78029	test's l1: 4.26739
[720]	train's l1: 2.77185	test's l1: 4.25885
[730]	train's l1: 2.7608	test's l1: 4.25579
[740]	train's l1: 2.7002	test's l1: 4.19426
[750]	train's l1: 2.66253	test's l1: 4.18028
[760]	train's l1: 2.6612	test's l1: 4.18018
[770]	train's l1: 2.65939	test's l1: 4.1802
[780]	train's l1: 2.65848	test's l1: 4.18014
[790]	train's l1: 2.65736	test's l1: 4.17996
[800]	train's l1: 2.65676	test's l1: 4.18016
[810]	train's l1: 2.6557	test's l1: 4.18094
[820]	train's l1: 2.65391	test's l1: 4.18069
[830]	train's l1: 2.6516	test's l1: 4.17817
[840]	train's l1: 2.64786	test's l1: 4.17724
[850]	train's l1: 2.64443	test's l1: 4.17616
[860]	train's l1: 2.63913	test's l1: 4.17469
[870]	train's l1: 2.58561	test's l1: 4.10996
[880]	train's l1: 2.58501	test's l1: 4.10989
[890]	train's l1: 2.57949	test's l1: 4.11339
[900]	train's l1: 2.56414	test's l1: 4.10045
[910]	train's l1: 2.55687	test's l1: 4.09841
[920]	train's l1: 2.55587	test's l1: 4.09821
[930]	train's l1: 2.55572	test's l1: 4.09822
[940]	train's l1: 2.5554	test's l1: 4.09836
[950]	train's l1: 2.55469	test's l1: 4.09785
[960]	train's l1: 2.54882	test's l1: 4.09571
[970]	train's l1: 2.51457	test's l1: 4.06442
[980]	train's l1: 2.51261	test's l1: 4.06355
[990]	train's l1: 2.49264	test's l1: 4.03185
[1000]	train's l1: 2.49033	test's l1: 4.03113
Did not meet early stopping. Best iteration is:
[997]	train's l1: 2.49117	test's l1: 4.03025
Starting for w150_False with mul=10
150: 52m30sec done
150: 52m40sec done
150: 52m50sec done
150: 53m0sec done
150: 53m10sec done
150: 53m20sec done
150: 53m30sec done
150: 53m40sec done
150: 53m50sec done
150: 54m0sec done
150: 54m10sec done
150: 54m20sec done
150: 54m30sec done
150: 54m40sec done
150: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.202363 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60275
[LightGBM] [Info] Number of data points in the train set: 1520400, number of used features: 246
[LightGBM] [Info] Start training from score 71.900002
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4009	test's l1: 61.3761
[20]	train's l1: 39.9119	test's l1: 39.976
[30]	train's l1: 26.459	test's l1: 26.5593
[40]	train's l1: 18.3829	test's l1: 18.7373
[50]	train's l1: 11.1036	test's l1: 11.4617
[60]	train's l1: 7.34911	test's l1: 7.61415
[70]	train's l1: 6.01397	test's l1: 6.60443
[80]	train's l1: 5.13366	test's l1: 5.86948
[90]	train's l1: 4.58798	test's l1: 5.31605
[100]	train's l1: 4.42388	test's l1: 5.11753
[110]	train's l1: 4.40384	test's l1: 5.09095
[120]	train's l1: 4.38063	test's l1: 5.06865
[130]	train's l1: 4.36697	test's l1: 5.05451
[140]	train's l1: 4.2828	test's l1: 5.04545
[150]	train's l1: 4.24557	test's l1: 5.02958
[160]	train's l1: 4.21364	test's l1: 4.99896
[170]	train's l1: 4.16618	test's l1: 4.94418
[180]	train's l1: 4.0842	test's l1: 4.86078
[190]	train's l1: 3.96413	test's l1: 4.7901
[200]	train's l1: 3.81107	test's l1: 4.72043
[210]	train's l1: 3.37978	test's l1: 4.46171
[220]	train's l1: 3.13362	test's l1: 4.34621
[230]	train's l1: 3.08013	test's l1: 4.32506
[240]	train's l1: 3.06972	test's l1: 4.31704
[250]	train's l1: 3.06193	test's l1: 4.31358
[260]	train's l1: 3.05403	test's l1: 4.30896
[270]	train's l1: 3.03823	test's l1: 4.29277
[280]	train's l1: 3.02301	test's l1: 4.28941
[290]	train's l1: 3.0027	test's l1: 4.27844
[300]	train's l1: 2.99459	test's l1: 4.27322
[310]	train's l1: 2.99292	test's l1: 4.2724
[320]	train's l1: 2.98371	test's l1: 4.26302
[330]	train's l1: 2.97154	test's l1: 4.26176
[340]	train's l1: 2.96471	test's l1: 4.2582
[350]	train's l1: 2.95863	test's l1: 4.25416
[360]	train's l1: 2.95544	test's l1: 4.25274
[370]	train's l1: 2.95285	test's l1: 4.25334
[380]	train's l1: 2.94366	test's l1: 4.25319
[390]	train's l1: 2.94296	test's l1: 4.25291
[400]	train's l1: 2.94227	test's l1: 4.25291
[410]	train's l1: 2.94152	test's l1: 4.25298
[420]	train's l1: 2.93932	test's l1: 4.25227
[430]	train's l1: 2.89965	test's l1: 4.22493
[440]	train's l1: 2.88284	test's l1: 4.21168
[450]	train's l1: 2.88173	test's l1: 4.21066
[460]	train's l1: 2.88074	test's l1: 4.21062
[470]	train's l1: 2.84326	test's l1: 4.17561
[480]	train's l1: 2.84153	test's l1: 4.17466
[490]	train's l1: 2.83984	test's l1: 4.1748
[500]	train's l1: 2.8364	test's l1: 4.1751
[510]	train's l1: 2.79942	test's l1: 4.13366
[520]	train's l1: 2.79807	test's l1: 4.13362
[530]	train's l1: 2.79687	test's l1: 4.13298
[540]	train's l1: 2.79303	test's l1: 4.13386
[550]	train's l1: 2.79115	test's l1: 4.13197
[560]	train's l1: 2.78872	test's l1: 4.13202
[570]	train's l1: 2.75485	test's l1: 4.05621
[580]	train's l1: 2.75277	test's l1: 4.05486
[590]	train's l1: 2.74754	test's l1: 4.05425
[600]	train's l1: 2.73498	test's l1: 4.03885
[610]	train's l1: 2.71552	test's l1: 4.02011
[620]	train's l1: 2.6897	test's l1: 4.01785
[630]	train's l1: 2.6683	test's l1: 4.01432
[640]	train's l1: 2.64303	test's l1: 3.98266
[650]	train's l1: 2.62599	test's l1: 3.97157
[660]	train's l1: 2.62559	test's l1: 3.97133
[670]	train's l1: 2.57616	test's l1: 3.93143
[680]	train's l1: 2.57416	test's l1: 3.93174
[690]	train's l1: 2.57375	test's l1: 3.93144
[700]	train's l1: 2.57109	test's l1: 3.93198
[710]	train's l1: 2.56629	test's l1: 3.92816
[720]	train's l1: 2.56023	test's l1: 3.9201
[730]	train's l1: 2.55807	test's l1: 3.91814
[740]	train's l1: 2.55711	test's l1: 3.91737
[750]	train's l1: 2.55445	test's l1: 3.91513
[760]	train's l1: 2.55269	test's l1: 3.91541
[770]	train's l1: 2.55233	test's l1: 3.91531
[780]	train's l1: 2.54965	test's l1: 3.91603
[790]	train's l1: 2.54878	test's l1: 3.91668
[800]	train's l1: 2.54776	test's l1: 3.91721
[810]	train's l1: 2.54567	test's l1: 3.91706
[820]	train's l1: 2.54526	test's l1: 3.91684
[830]	train's l1: 2.54099	test's l1: 3.91597
[840]	train's l1: 2.53994	test's l1: 3.91588
[850]	train's l1: 2.53228	test's l1: 3.91628
[860]	train's l1: 2.53141	test's l1: 3.91372
[870]	train's l1: 2.52903	test's l1: 3.91253
[880]	train's l1: 2.52217	test's l1: 3.90712
[890]	train's l1: 2.51946	test's l1: 3.90642
[900]	train's l1: 2.51795	test's l1: 3.90523
[910]	train's l1: 2.51692	test's l1: 3.90476
[920]	train's l1: 2.46801	test's l1: 3.86997
[930]	train's l1: 2.46653	test's l1: 3.87025
[940]	train's l1: 2.4634	test's l1: 3.86991
[950]	train's l1: 2.46241	test's l1: 3.8703
[960]	train's l1: 2.45808	test's l1: 3.86898
[970]	train's l1: 2.4577	test's l1: 3.869
[980]	train's l1: 2.44228	test's l1: 3.84814
[990]	train's l1: 2.42938	test's l1: 3.82778
[1000]	train's l1: 2.42898	test's l1: 3.8278
Did not meet early stopping. Best iteration is:
[992]	train's l1: 2.42934	test's l1: 3.82778
Starting for w100_False with mul=10
100: 53m20sec done
100: 53m30sec done
100: 53m40sec done
100: 53m50sec done
100: 54m0sec done
100: 54m10sec done
100: 54m20sec done
100: 54m30sec done
100: 54m40sec done
100: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.307953 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1940400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4848	test's l1: 61.4265
[20]	train's l1: 39.9901	test's l1: 39.9784
[30]	train's l1: 26.5464	test's l1: 26.5797
[40]	train's l1: 18.283	test's l1: 18.554
[50]	train's l1: 11.2788	test's l1: 11.2816
[60]	train's l1: 7.95461	test's l1: 7.92661
[70]	train's l1: 5.91212	test's l1: 6.22699
[80]	train's l1: 5.22402	test's l1: 5.77303
[90]	train's l1: 4.44979	test's l1: 5.02367
[100]	train's l1: 4.20884	test's l1: 4.75437
[110]	train's l1: 4.0231	test's l1: 4.54462
[120]	train's l1: 3.99121	test's l1: 4.51523
[130]	train's l1: 3.86245	test's l1: 4.43054
[140]	train's l1: 3.79943	test's l1: 4.37864
[150]	train's l1: 3.73298	test's l1: 4.3146
[160]	train's l1: 3.57568	test's l1: 4.20456
[170]	train's l1: 3.54534	test's l1: 4.1901
[180]	train's l1: 3.49498	test's l1: 4.16031
[190]	train's l1: 3.42583	test's l1: 4.11443
[200]	train's l1: 3.38749	test's l1: 4.08383
[210]	train's l1: 3.31853	test's l1: 4.04383
[220]	train's l1: 3.27317	test's l1: 4.00904
[230]	train's l1: 3.25298	test's l1: 3.99026
[240]	train's l1: 3.20475	test's l1: 3.95065
[250]	train's l1: 3.20278	test's l1: 3.94952
[260]	train's l1: 3.19614	test's l1: 3.9504
[270]	train's l1: 3.19308	test's l1: 3.94813
[280]	train's l1: 3.07973	test's l1: 3.90746
[290]	train's l1: 3.07829	test's l1: 3.90707
[300]	train's l1: 3.07218	test's l1: 3.90745
[310]	train's l1: 3.06257	test's l1: 3.90672
[320]	train's l1: 3.00697	test's l1: 3.8741
[330]	train's l1: 2.87797	test's l1: 3.79443
[340]	train's l1: 2.87394	test's l1: 3.79304
[350]	train's l1: 2.85114	test's l1: 3.7739
[360]	train's l1: 2.84676	test's l1: 3.7744
[370]	train's l1: 2.83981	test's l1: 3.77274
[380]	train's l1: 2.83045	test's l1: 3.76928
[390]	train's l1: 2.8239	test's l1: 3.76207
[400]	train's l1: 2.81712	test's l1: 3.75603
[410]	train's l1: 2.81565	test's l1: 3.75635
[420]	train's l1: 2.81327	test's l1: 3.75543
[430]	train's l1: 2.80636	test's l1: 3.75292
[440]	train's l1: 2.8002	test's l1: 3.74958
[450]	train's l1: 2.78813	test's l1: 3.74347
[460]	train's l1: 2.7285	test's l1: 3.69317
[470]	train's l1: 2.66466	test's l1: 3.67054
[480]	train's l1: 2.6534	test's l1: 3.66468
[490]	train's l1: 2.63835	test's l1: 3.65874
[500]	train's l1: 2.59795	test's l1: 3.62861
[510]	train's l1: 2.58878	test's l1: 3.62262
[520]	train's l1: 2.52375	test's l1: 3.5711
[530]	train's l1: 2.49094	test's l1: 3.54548
[540]	train's l1: 2.48698	test's l1: 3.53807
[550]	train's l1: 2.4833	test's l1: 3.53739
[560]	train's l1: 2.47796	test's l1: 3.53164
[570]	train's l1: 2.47695	test's l1: 3.53202
[580]	train's l1: 2.47514	test's l1: 3.5304
[590]	train's l1: 2.46776	test's l1: 3.52807
[600]	train's l1: 2.45446	test's l1: 3.52161
[610]	train's l1: 2.41677	test's l1: 3.50166
[620]	train's l1: 2.41244	test's l1: 3.50162
[630]	train's l1: 2.40951	test's l1: 3.50059
[640]	train's l1: 2.40695	test's l1: 3.49868
[650]	train's l1: 2.40505	test's l1: 3.49656
[660]	train's l1: 2.40194	test's l1: 3.49624
[670]	train's l1: 2.4015	test's l1: 3.49612
[680]	train's l1: 2.37893	test's l1: 3.48613
[690]	train's l1: 2.37518	test's l1: 3.48468
[700]	train's l1: 2.36989	test's l1: 3.48379
[710]	train's l1: 2.36461	test's l1: 3.48156
[720]	train's l1: 2.36409	test's l1: 3.48154
[730]	train's l1: 2.35455	test's l1: 3.47272
[740]	train's l1: 2.34847	test's l1: 3.46993
[750]	train's l1: 2.34567	test's l1: 3.46903
[760]	train's l1: 2.33165	test's l1: 3.4586
[770]	train's l1: 2.32959	test's l1: 3.45752
[780]	train's l1: 2.3001	test's l1: 3.42942
[790]	train's l1: 2.29904	test's l1: 3.42871
[800]	train's l1: 2.29596	test's l1: 3.42785
[810]	train's l1: 2.2925	test's l1: 3.42597
[820]	train's l1: 2.28473	test's l1: 3.42409
[830]	train's l1: 2.28366	test's l1: 3.42409
[840]	train's l1: 2.27981	test's l1: 3.42177
[850]	train's l1: 2.27785	test's l1: 3.42136
[860]	train's l1: 2.27541	test's l1: 3.42291
[870]	train's l1: 2.27452	test's l1: 3.42277
[880]	train's l1: 2.26961	test's l1: 3.42088
[890]	train's l1: 2.26884	test's l1: 3.42059
[900]	train's l1: 2.26478	test's l1: 3.41918
[910]	train's l1: 2.24599	test's l1: 3.39601
[920]	train's l1: 2.24204	test's l1: 3.39656
[930]	train's l1: 2.24078	test's l1: 3.39632
[940]	train's l1: 2.23776	test's l1: 3.39604
[950]	train's l1: 2.2354	test's l1: 3.39461
[960]	train's l1: 2.23419	test's l1: 3.39456
[970]	train's l1: 2.23366	test's l1: 3.39436
[980]	train's l1: 2.23149	test's l1: 3.39403
[990]	train's l1: 2.22589	test's l1: 3.39035
[1000]	train's l1: 2.22323	test's l1: 3.38941
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.22323	test's l1: 3.38941
Starting for w50_False with mul=10
