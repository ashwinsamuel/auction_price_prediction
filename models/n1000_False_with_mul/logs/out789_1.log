0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
1
2
3
4
5
6
0
Starting for w200_False with mul=7
200: 51m40sec done
200: 51m50sec done
200: 52m0sec done
200: 52m10sec done
200: 52m20sec done
200: 52m30sec done
200: 52m40sec done
200: 52m50sec done
200: 53m0sec done
200: 53m10sec done
200: 53m20sec done
200: 53m30sec done
200: 53m40sec done
200: 53m50sec done
200: 54m0sec done
200: 54m10sec done
200: 54m20sec done
200: 54m30sec done
200: 54m40sec done
200: 54m50sec done
python(15625) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.158193 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1100400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4823	test's l1: 61.4529
[20]	train's l1: 39.8675	test's l1: 39.9389
[30]	train's l1: 26.504	test's l1: 26.5988
[40]	train's l1: 18.3469	test's l1: 18.6854
[50]	train's l1: 11.1845	test's l1: 11.4797
[60]	train's l1: 7.58437	test's l1: 8.03456
[70]	train's l1: 5.98428	test's l1: 6.6469
[80]	train's l1: 5.29505	test's l1: 6.02658
[90]	train's l1: 4.49529	test's l1: 5.29493
[100]	train's l1: 4.36792	test's l1: 5.16733
[110]	train's l1: 4.2983	test's l1: 5.0712
[120]	train's l1: 4.29602	test's l1: 5.07601
[130]	train's l1: 4.2919	test's l1: 5.08586
[140]	train's l1: 4.2828	test's l1: 5.07684
[150]	train's l1: 4.28038	test's l1: 5.07508
[160]	train's l1: 4.20365	test's l1: 5.0351
[170]	train's l1: 4.15304	test's l1: 5.01699
[180]	train's l1: 3.98727	test's l1: 4.95191
[190]	train's l1: 3.69004	test's l1: 4.76571
[200]	train's l1: 3.68661	test's l1: 4.7664
[210]	train's l1: 3.65436	test's l1: 4.7489
[220]	train's l1: 3.64617	test's l1: 4.73992
[230]	train's l1: 3.63905	test's l1: 4.7323
[240]	train's l1: 3.63195	test's l1: 4.72323
[250]	train's l1: 3.62386	test's l1: 4.71572
[260]	train's l1: 3.61416	test's l1: 4.71348
[270]	train's l1: 3.56237	test's l1: 4.65091
[280]	train's l1: 3.50308	test's l1: 4.63025
[290]	train's l1: 3.50093	test's l1: 4.62766
[300]	train's l1: 3.45041	test's l1: 4.61177
[310]	train's l1: 3.39811	test's l1: 4.56094
[320]	train's l1: 3.39062	test's l1: 4.55745
[330]	train's l1: 3.3867	test's l1: 4.55565
[340]	train's l1: 3.37993	test's l1: 4.5521
[350]	train's l1: 3.37754	test's l1: 4.55166
[360]	train's l1: 3.37547	test's l1: 4.5508
[370]	train's l1: 3.37125	test's l1: 4.54877
[380]	train's l1: 3.35571	test's l1: 4.53621
[390]	train's l1: 3.32568	test's l1: 4.5241
[400]	train's l1: 3.3148	test's l1: 4.52194
[410]	train's l1: 3.25517	test's l1: 4.47217
[420]	train's l1: 3.2463	test's l1: 4.47363
[430]	train's l1: 3.24486	test's l1: 4.47387
[440]	train's l1: 3.2383	test's l1: 4.46583
[450]	train's l1: 3.22822	test's l1: 4.47592
[460]	train's l1: 3.21983	test's l1: 4.47038
[470]	train's l1: 3.06662	test's l1: 4.36288
[480]	train's l1: 2.92153	test's l1: 4.30482
[490]	train's l1: 2.87859	test's l1: 4.26417
[500]	train's l1: 2.80955	test's l1: 4.23354
[510]	train's l1: 2.76377	test's l1: 4.20572
[520]	train's l1: 2.7621	test's l1: 4.20413
[530]	train's l1: 2.75897	test's l1: 4.20197
[540]	train's l1: 2.75785	test's l1: 4.20228
[550]	train's l1: 2.74922	test's l1: 4.19361
[560]	train's l1: 2.74063	test's l1: 4.18244
[570]	train's l1: 2.70236	test's l1: 4.14476
[580]	train's l1: 2.68959	test's l1: 4.12714
[590]	train's l1: 2.68548	test's l1: 4.12714
[600]	train's l1: 2.68073	test's l1: 4.12561
[610]	train's l1: 2.67013	test's l1: 4.1249
[620]	train's l1: 2.66841	test's l1: 4.12419
[630]	train's l1: 2.66648	test's l1: 4.12265
[640]	train's l1: 2.66605	test's l1: 4.12267
[650]	train's l1: 2.64288	test's l1: 4.09434
[660]	train's l1: 2.64246	test's l1: 4.09437
[670]	train's l1: 2.63171	test's l1: 4.09024
[680]	train's l1: 2.62896	test's l1: 4.08883
[690]	train's l1: 2.62215	test's l1: 4.08331
[700]	train's l1: 2.61305	test's l1: 4.0803
[710]	train's l1: 2.46484	test's l1: 3.9765
[720]	train's l1: 2.46202	test's l1: 3.97499
[730]	train's l1: 2.46085	test's l1: 3.9779
[740]	train's l1: 2.45902	test's l1: 3.97838
[750]	train's l1: 2.45737	test's l1: 3.97829
[760]	train's l1: 2.45569	test's l1: 3.97765
[770]	train's l1: 2.44492	test's l1: 3.97072
[780]	train's l1: 2.36069	test's l1: 3.91606
[790]	train's l1: 2.32691	test's l1: 3.88142
[800]	train's l1: 2.31665	test's l1: 3.88021
[810]	train's l1: 2.26281	test's l1: 3.83426
[820]	train's l1: 2.25473	test's l1: 3.8185
[830]	train's l1: 2.2528	test's l1: 3.81747
[840]	train's l1: 2.2517	test's l1: 3.81859
[850]	train's l1: 2.25022	test's l1: 3.81788
[860]	train's l1: 2.24934	test's l1: 3.81722
[870]	train's l1: 2.24871	test's l1: 3.81678
[880]	train's l1: 2.24836	test's l1: 3.81683
[890]	train's l1: 2.24783	test's l1: 3.81681
[900]	train's l1: 2.24602	test's l1: 3.81664
[910]	train's l1: 2.24471	test's l1: 3.81576
[920]	train's l1: 2.24128	test's l1: 3.81411
[930]	train's l1: 2.24058	test's l1: 3.81421
[940]	train's l1: 2.23782	test's l1: 3.81216
[950]	train's l1: 2.23489	test's l1: 3.81182
[960]	train's l1: 2.23254	test's l1: 3.81173
[970]	train's l1: 2.2207	test's l1: 3.80487
[980]	train's l1: 2.22023	test's l1: 3.80491
[990]	train's l1: 2.21851	test's l1: 3.80357
[1000]	train's l1: 2.21272	test's l1: 3.80026
Did not meet early stopping. Best iteration is:
[998]	train's l1: 2.21282	test's l1: 3.80022
Starting for w150_False with mul=7
150: 52m30sec done
150: 52m40sec done
150: 52m50sec done
150: 53m0sec done
150: 53m10sec done
150: 53m20sec done
150: 53m30sec done
150: 53m40sec done
150: 53m50sec done
150: 54m0sec done
150: 54m10sec done
150: 54m20sec done
150: 54m30sec done
150: 54m40sec done
150: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224904 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1520400, number of used features: 247
[LightGBM] [Info] Start training from score 71.900002
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.3994	test's l1: 61.374
[20]	train's l1: 39.9086	test's l1: 39.9742
[30]	train's l1: 26.4489	test's l1: 26.5511
[40]	train's l1: 18.3971	test's l1: 18.7503
[50]	train's l1: 11.1405	test's l1: 11.5026
[60]	train's l1: 7.30895	test's l1: 7.68612
[70]	train's l1: 5.99898	test's l1: 6.63884
[80]	train's l1: 4.61312	test's l1: 5.41239
[90]	train's l1: 4.34134	test's l1: 5.09998
[100]	train's l1: 4.2642	test's l1: 4.9914
[110]	train's l1: 4.24034	test's l1: 4.96784
[120]	train's l1: 4.18446	test's l1: 4.932
[130]	train's l1: 4.09266	test's l1: 4.8466
[140]	train's l1: 3.98506	test's l1: 4.7867
[150]	train's l1: 3.98051	test's l1: 4.78375
[160]	train's l1: 3.95343	test's l1: 4.76435
[170]	train's l1: 3.90648	test's l1: 4.71373
[180]	train's l1: 3.89358	test's l1: 4.71166
[190]	train's l1: 3.70354	test's l1: 4.57873
[200]	train's l1: 3.65977	test's l1: 4.53167
[210]	train's l1: 3.64525	test's l1: 4.52277
[220]	train's l1: 3.62588	test's l1: 4.51182
[230]	train's l1: 3.60567	test's l1: 4.48774
[240]	train's l1: 3.50625	test's l1: 4.43414
[250]	train's l1: 3.49939	test's l1: 4.43248
[260]	train's l1: 3.49094	test's l1: 4.42345
[270]	train's l1: 3.49003	test's l1: 4.42302
[280]	train's l1: 3.48867	test's l1: 4.42282
[290]	train's l1: 3.4305	test's l1: 4.38199
[300]	train's l1: 3.42939	test's l1: 4.38134
[310]	train's l1: 3.35367	test's l1: 4.30548
[320]	train's l1: 3.34528	test's l1: 4.30289
[330]	train's l1: 3.33977	test's l1: 4.29823
[340]	train's l1: 3.33435	test's l1: 4.30027
[350]	train's l1: 3.23965	test's l1: 4.26012
[360]	train's l1: 3.20992	test's l1: 4.24406
[370]	train's l1: 3.17024	test's l1: 4.22393
[380]	train's l1: 3.15695	test's l1: 4.21033
[390]	train's l1: 3.15076	test's l1: 4.21138
[400]	train's l1: 3.08947	test's l1: 4.19734
[410]	train's l1: 3.05594	test's l1: 4.17496
[420]	train's l1: 3.04884	test's l1: 4.17392
[430]	train's l1: 3.02002	test's l1: 4.15185
[440]	train's l1: 3.01797	test's l1: 4.15115
[450]	train's l1: 2.98097	test's l1: 4.12609
[460]	train's l1: 2.97828	test's l1: 4.12554
[470]	train's l1: 2.90975	test's l1: 4.09398
[480]	train's l1: 2.9069	test's l1: 4.09163
[490]	train's l1: 2.89856	test's l1: 4.09173
[500]	train's l1: 2.88314	test's l1: 4.08354
[510]	train's l1: 2.87188	test's l1: 4.08175
[520]	train's l1: 2.86323	test's l1: 4.0902
[530]	train's l1: 2.85872	test's l1: 4.08924
[540]	train's l1: 2.8574	test's l1: 4.08841
[550]	train's l1: 2.78609	test's l1: 4.03233
[560]	train's l1: 2.78055	test's l1: 4.03439
[570]	train's l1: 2.68897	test's l1: 3.9614
[580]	train's l1: 2.6883	test's l1: 3.96079
[590]	train's l1: 2.66534	test's l1: 3.93845
[600]	train's l1: 2.66253	test's l1: 3.93796
[610]	train's l1: 2.65187	test's l1: 3.93169
[620]	train's l1: 2.64775	test's l1: 3.92552
[630]	train's l1: 2.63556	test's l1: 3.92184
[640]	train's l1: 2.62048	test's l1: 3.91864
[650]	train's l1: 2.61018	test's l1: 3.91505
[660]	train's l1: 2.60236	test's l1: 3.90927
[670]	train's l1: 2.55187	test's l1: 3.87307
[680]	train's l1: 2.53931	test's l1: 3.85274
[690]	train's l1: 2.53164	test's l1: 3.84685
[700]	train's l1: 2.46948	test's l1: 3.79314
[710]	train's l1: 2.44003	test's l1: 3.76853
[720]	train's l1: 2.43836	test's l1: 3.7677
[730]	train's l1: 2.43774	test's l1: 3.76784
[740]	train's l1: 2.43385	test's l1: 3.76789
[750]	train's l1: 2.43023	test's l1: 3.76808
[760]	train's l1: 2.42258	test's l1: 3.7652
[770]	train's l1: 2.42117	test's l1: 3.76564
[780]	train's l1: 2.38478	test's l1: 3.73418
[790]	train's l1: 2.38421	test's l1: 3.73406
[800]	train's l1: 2.38314	test's l1: 3.73337
[810]	train's l1: 2.38191	test's l1: 3.73288
[820]	train's l1: 2.38087	test's l1: 3.73168
[830]	train's l1: 2.37897	test's l1: 3.73104
[840]	train's l1: 2.37816	test's l1: 3.73064
[850]	train's l1: 2.37766	test's l1: 3.73061
[860]	train's l1: 2.35773	test's l1: 3.70433
[870]	train's l1: 2.34763	test's l1: 3.68737
[880]	train's l1: 2.34554	test's l1: 3.68588
[890]	train's l1: 2.32199	test's l1: 3.68316
[900]	train's l1: 2.32151	test's l1: 3.68311
[910]	train's l1: 2.31991	test's l1: 3.68274
[920]	train's l1: 2.31771	test's l1: 3.68219
[930]	train's l1: 2.31433	test's l1: 3.6797
[940]	train's l1: 2.31378	test's l1: 3.67951
[950]	train's l1: 2.31303	test's l1: 3.67965
[960]	train's l1: 2.31172	test's l1: 3.67901
[970]	train's l1: 2.3098	test's l1: 3.67871
[980]	train's l1: 2.30603	test's l1: 3.67773
[990]	train's l1: 2.30564	test's l1: 3.6777
[1000]	train's l1: 2.29316	test's l1: 3.65781
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.29316	test's l1: 3.65781
Starting for w100_False with mul=7
100: 53m20sec done
100: 53m30sec done
100: 53m40sec done
100: 53m50sec done
100: 54m0sec done
100: 54m10sec done
100: 54m20sec done
100: 54m30sec done
100: 54m40sec done
100: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.227438 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1940400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4825	test's l1: 61.4219
[20]	train's l1: 39.9958	test's l1: 39.9865
[30]	train's l1: 26.5554	test's l1: 26.589
[40]	train's l1: 18.2891	test's l1: 18.5629
[50]	train's l1: 11.2633	test's l1: 11.2649
[60]	train's l1: 7.90823	test's l1: 7.89741
[70]	train's l1: 6.32882	test's l1: 6.5987
[80]	train's l1: 5.32903	test's l1: 5.66772
[90]	train's l1: 4.13673	test's l1: 4.92505
[100]	train's l1: 3.97893	test's l1: 4.73507
[110]	train's l1: 3.86781	test's l1: 4.64663
[120]	train's l1: 3.7539	test's l1: 4.52156
[130]	train's l1: 3.73268	test's l1: 4.50473
[140]	train's l1: 3.66579	test's l1: 4.3265
[150]	train's l1: 3.54517	test's l1: 4.26068
[160]	train's l1: 3.54327	test's l1: 4.2599
[170]	train's l1: 3.54109	test's l1: 4.25864
[180]	train's l1: 3.53185	test's l1: 4.25434
[190]	train's l1: 3.51681	test's l1: 4.24664
[200]	train's l1: 3.36466	test's l1: 4.1493
[210]	train's l1: 3.22083	test's l1: 4.08784
[220]	train's l1: 3.14099	test's l1: 4.05587
[230]	train's l1: 3.11029	test's l1: 4.0332
[240]	train's l1: 3.0913	test's l1: 4.02088
[250]	train's l1: 3.08453	test's l1: 4.01529
[260]	train's l1: 3.00849	test's l1: 3.90915
[270]	train's l1: 3.00396	test's l1: 3.90974
[280]	train's l1: 2.9798	test's l1: 3.90691
[290]	train's l1: 2.97006	test's l1: 3.90786
[300]	train's l1: 2.96398	test's l1: 3.90402
[310]	train's l1: 2.93531	test's l1: 3.88246
[320]	train's l1: 2.93014	test's l1: 3.87983
[330]	train's l1: 2.92183	test's l1: 3.87607
[340]	train's l1: 2.91255	test's l1: 3.87416
[350]	train's l1: 2.91209	test's l1: 3.87412
[360]	train's l1: 2.91061	test's l1: 3.87377
[370]	train's l1: 2.90838	test's l1: 3.87373
[380]	train's l1: 2.90496	test's l1: 3.87514
[390]	train's l1: 2.86183	test's l1: 3.84606
[400]	train's l1: 2.85977	test's l1: 3.84578
[410]	train's l1: 2.85946	test's l1: 3.84555
[420]	train's l1: 2.85791	test's l1: 3.84494
[430]	train's l1: 2.84112	test's l1: 3.83374
[440]	train's l1: 2.78078	test's l1: 3.74608
[450]	train's l1: 2.73013	test's l1: 3.71486
[460]	train's l1: 2.70306	test's l1: 3.68325
[470]	train's l1: 2.70238	test's l1: 3.68296
[480]	train's l1: 2.6403	test's l1: 3.65842
[490]	train's l1: 2.6214	test's l1: 3.63913
[500]	train's l1: 2.61441	test's l1: 3.63415
[510]	train's l1: 2.54902	test's l1: 3.59973
[520]	train's l1: 2.53826	test's l1: 3.59598
[530]	train's l1: 2.53425	test's l1: 3.59598
[540]	train's l1: 2.52238	test's l1: 3.59235
[550]	train's l1: 2.52181	test's l1: 3.59208
[560]	train's l1: 2.51542	test's l1: 3.59008
[570]	train's l1: 2.51419	test's l1: 3.58955
[580]	train's l1: 2.51215	test's l1: 3.58854
[590]	train's l1: 2.50088	test's l1: 3.58244
[600]	train's l1: 2.48207	test's l1: 3.5715
[610]	train's l1: 2.47553	test's l1: 3.56568
[620]	train's l1: 2.45974	test's l1: 3.55464
[630]	train's l1: 2.35738	test's l1: 3.45715
[640]	train's l1: 2.33044	test's l1: 3.43892
[650]	train's l1: 2.3165	test's l1: 3.43302
[660]	train's l1: 2.31302	test's l1: 3.43185
[670]	train's l1: 2.31178	test's l1: 3.43165
[680]	train's l1: 2.31061	test's l1: 3.43092
[690]	train's l1: 2.3076	test's l1: 3.4298
[700]	train's l1: 2.30555	test's l1: 3.42783
[710]	train's l1: 2.29814	test's l1: 3.4262
[720]	train's l1: 2.29768	test's l1: 3.42612
[730]	train's l1: 2.29691	test's l1: 3.42562
[740]	train's l1: 2.29539	test's l1: 3.42566
[750]	train's l1: 2.29259	test's l1: 3.42441
[760]	train's l1: 2.28987	test's l1: 3.42417
[770]	train's l1: 2.28885	test's l1: 3.42406
[780]	train's l1: 2.26605	test's l1: 3.39407
[790]	train's l1: 2.26253	test's l1: 3.39367
[800]	train's l1: 2.26039	test's l1: 3.3896
[810]	train's l1: 2.23731	test's l1: 3.37683
[820]	train's l1: 2.23489	test's l1: 3.37612
[830]	train's l1: 2.23361	test's l1: 3.37423
[840]	train's l1: 2.233	test's l1: 3.3741
[850]	train's l1: 2.2324	test's l1: 3.37389
[860]	train's l1: 2.23095	test's l1: 3.37336
[870]	train's l1: 2.22561	test's l1: 3.36772
[880]	train's l1: 2.21695	test's l1: 3.36224
[890]	train's l1: 2.21449	test's l1: 3.35883
[900]	train's l1: 2.21121	test's l1: 3.35554
[910]	train's l1: 2.20913	test's l1: 3.35533
[920]	train's l1: 2.20204	test's l1: 3.35145
[930]	train's l1: 2.19712	test's l1: 3.34802
[940]	train's l1: 2.1864	test's l1: 3.3386
[950]	train's l1: 2.18538	test's l1: 3.33843
[960]	train's l1: 2.18482	test's l1: 3.33834
[970]	train's l1: 2.18463	test's l1: 3.3383
[980]	train's l1: 2.17782	test's l1: 3.33267
[990]	train's l1: 2.17669	test's l1: 3.33184
[1000]	train's l1: 2.17045	test's l1: 3.32786
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.17045	test's l1: 3.32786
Starting for w50_False with mul=7
50: 54m10sec done
50: 54m20sec done
50: 54m30sec done
50: 54m40sec done
50: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.319643 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 2360400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.3785	test's l1: 61.3184
[20]	train's l1: 39.8769	test's l1: 39.9216
[30]	train's l1: 26.4001	test's l1: 26.4798
[40]	train's l1: 18.6996	test's l1: 18.9898
[50]	train's l1: 11.4113	test's l1: 11.5795
[60]	train's l1: 8.75774	test's l1: 9.06348
[70]	train's l1: 7.35652	test's l1: 7.82201
[80]	train's l1: 5.52036	test's l1: 6.10633
[90]	train's l1: 4.67042	test's l1: 5.57358
[100]	train's l1: 4.54912	test's l1: 5.46481
[110]	train's l1: 3.95416	test's l1: 4.96123
[120]	train's l1: 3.93752	test's l1: 4.9471
[130]	train's l1: 3.60276	test's l1: 4.64757
[140]	train's l1: 3.51199	test's l1: 4.5888
[150]	train's l1: 3.35296	test's l1: 4.4779
[160]	train's l1: 3.31704	test's l1: 4.4565
[170]	train's l1: 3.30807	test's l1: 4.44446
[180]	train's l1: 3.30108	test's l1: 4.4415
[190]	train's l1: 3.24312	test's l1: 4.36956
[200]	train's l1: 3.23062	test's l1: 4.35076
[210]	train's l1: 3.2233	test's l1: 4.34607
[220]	train's l1: 3.17919	test's l1: 4.31471
[230]	train's l1: 3.1769	test's l1: 4.31315
[240]	train's l1: 3.17159	test's l1: 4.3083
[250]	train's l1: 3.12171	test's l1: 4.26981
[260]	train's l1: 3.09507	test's l1: 4.22893
[270]	train's l1: 3.08291	test's l1: 4.21311
[280]	train's l1: 3.06526	test's l1: 4.19388
[290]	train's l1: 3.04898	test's l1: 4.19068
[300]	train's l1: 3.03816	test's l1: 4.17632
[310]	train's l1: 2.98874	test's l1: 4.15134
[320]	train's l1: 2.9687	test's l1: 4.13718
[330]	train's l1: 2.96048	test's l1: 4.12228
[340]	train's l1: 2.94804	test's l1: 4.11912
[350]	train's l1: 2.90047	test's l1: 4.07726
[360]	train's l1: 2.89909	test's l1: 4.07663
[370]	train's l1: 2.88624	test's l1: 4.07032
[380]	train's l1: 2.8739	test's l1: 4.0711
[390]	train's l1: 2.84856	test's l1: 4.0731
[400]	train's l1: 2.84515	test's l1: 4.07284
[410]	train's l1: 2.81465	test's l1: 4.05834
[420]	train's l1: 2.8021	test's l1: 4.05454
[430]	train's l1: 2.76007	test's l1: 4.02598
[440]	train's l1: 2.71815	test's l1: 3.98789
[450]	train's l1: 2.71365	test's l1: 3.98901
[460]	train's l1: 2.70562	test's l1: 3.98591
[470]	train's l1: 2.70484	test's l1: 3.98568
[480]	train's l1: 2.6573	test's l1: 3.93077
[490]	train's l1: 2.65573	test's l1: 3.93001
[500]	train's l1: 2.6544	test's l1: 3.92917
[510]	train's l1: 2.65138	test's l1: 3.92894
[520]	train's l1: 2.63702	test's l1: 3.91393
[530]	train's l1: 2.59687	test's l1: 3.87752
[540]	train's l1: 2.59599	test's l1: 3.87681
[550]	train's l1: 2.56568	test's l1: 3.85846
[560]	train's l1: 2.56398	test's l1: 3.85437
[570]	train's l1: 2.53141	test's l1: 3.83018
[580]	train's l1: 2.52681	test's l1: 3.82592
[590]	train's l1: 2.52485	test's l1: 3.82489
[600]	train's l1: 2.5239	test's l1: 3.82402
[610]	train's l1: 2.52298	test's l1: 3.82345
[620]	train's l1: 2.52032	test's l1: 3.82555
[630]	train's l1: 2.51846	test's l1: 3.82502
[640]	train's l1: 2.518	test's l1: 3.82488
[650]	train's l1: 2.51786	test's l1: 3.8248
[660]	train's l1: 2.51147	test's l1: 3.82292
[670]	train's l1: 2.50902	test's l1: 3.82412
[680]	train's l1: 2.50812	test's l1: 3.82377
[690]	train's l1: 2.50671	test's l1: 3.82344
[700]	train's l1: 2.49777	test's l1: 3.81765
[710]	train's l1: 2.49355	test's l1: 3.81654
[720]	train's l1: 2.49051	test's l1: 3.81579
[730]	train's l1: 2.48844	test's l1: 3.81595
[740]	train's l1: 2.48513	test's l1: 3.81583
[750]	train's l1: 2.4833	test's l1: 3.81468
[760]	train's l1: 2.47849	test's l1: 3.811
[770]	train's l1: 2.4773	test's l1: 3.81068
[780]	train's l1: 2.43232	test's l1: 3.73497
[790]	train's l1: 2.43077	test's l1: 3.73515
[800]	train's l1: 2.41759	test's l1: 3.69685
[810]	train's l1: 2.41714	test's l1: 3.69683
[820]	train's l1: 2.41473	test's l1: 3.69678
[830]	train's l1: 2.40163	test's l1: 3.68706
[840]	train's l1: 2.39712	test's l1: 3.68621
[850]	train's l1: 2.37686	test's l1: 3.66904
[860]	train's l1: 2.34575	test's l1: 3.65184
[870]	train's l1: 2.33665	test's l1: 3.6504
[880]	train's l1: 2.33656	test's l1: 3.65037
[890]	train's l1: 2.33604	test's l1: 3.65028
[900]	train's l1: 2.33463	test's l1: 3.64915
[910]	train's l1: 2.33301	test's l1: 3.64907
[920]	train's l1: 2.33154	test's l1: 3.64869
[930]	train's l1: 2.32067	test's l1: 3.63888
[940]	train's l1: 2.25214	test's l1: 3.60379
[950]	train's l1: 2.25064	test's l1: 3.60388
[960]	train's l1: 2.21686	test's l1: 3.57898
[970]	train's l1: 2.20339	test's l1: 3.56937
[980]	train's l1: 2.19763	test's l1: 3.56396
[990]	train's l1: 2.17957	test's l1: 3.55653
[1000]	train's l1: 2.17889	test's l1: 3.55623
Did not meet early stopping. Best iteration is:
[988]	train's l1: 2.18084	test's l1: 3.55619
Starting for w200_False with mul=8
200: 51m40sec done
200: 51m50sec done
200: 52m0sec done
200: 52m10sec done
200: 52m20sec done
200: 52m30sec done
200: 52m40sec done
200: 52m50sec done
200: 53m0sec done
200: 53m10sec done
200: 53m20sec done
200: 53m30sec done
200: 53m40sec done
200: 53m50sec done
200: 54m0sec done
200: 54m10sec done
200: 54m20sec done
200: 54m30sec done
200: 54m40sec done
200: 54m50sec done
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.434689 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 56187
[LightGBM] [Info] Number of data points in the train set: 1100400, number of used features: 228
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4824	test's l1: 61.4529
[20]	train's l1: 39.8702	test's l1: 39.9284
[30]	train's l1: 26.5119	test's l1: 26.5995
[40]	train's l1: 18.3551	test's l1: 18.6923
[50]	train's l1: 11.205	test's l1: 11.5064
[60]	train's l1: 7.52362	test's l1: 7.87622
[70]	train's l1: 5.91459	test's l1: 6.59659
[80]	train's l1: 4.66087	test's l1: 5.52335
[90]	train's l1: 4.51922	test's l1: 5.38422
[100]	train's l1: 4.43139	test's l1: 5.27764
[110]	train's l1: 4.35621	test's l1: 5.25285
[120]	train's l1: 4.28955	test's l1: 5.17872
[130]	train's l1: 4.21816	test's l1: 5.10762
[140]	train's l1: 4.20252	test's l1: 5.1009
[150]	train's l1: 4.13236	test's l1: 5.03927
[160]	train's l1: 4.08594	test's l1: 5.00834
[170]	train's l1: 4.07558	test's l1: 5.00326
[180]	train's l1: 4.06642	test's l1: 4.98881
[190]	train's l1: 3.98879	test's l1: 4.87709
[200]	train's l1: 3.86836	test's l1: 4.82284
[210]	train's l1: 3.68501	test's l1: 4.709
[220]	train's l1: 3.59867	test's l1: 4.64434
[230]	train's l1: 3.59207	test's l1: 4.64191
[240]	train's l1: 3.49621	test's l1: 4.60518
[250]	train's l1: 3.44727	test's l1: 4.57998
[260]	train's l1: 3.43718	test's l1: 4.57919
[270]	train's l1: 3.42949	test's l1: 4.57555
[280]	train's l1: 3.40117	test's l1: 4.56778
[290]	train's l1: 3.39643	test's l1: 4.56327
[300]	train's l1: 3.39394	test's l1: 4.56088
[310]	train's l1: 3.38402	test's l1: 4.56222
[320]	train's l1: 3.37586	test's l1: 4.55884
[330]	train's l1: 3.36947	test's l1: 4.5585
[340]	train's l1: 3.34247	test's l1: 4.54007
[350]	train's l1: 3.32847	test's l1: 4.52333
[360]	train's l1: 3.31149	test's l1: 4.51965
[370]	train's l1: 3.30926	test's l1: 4.51867
[380]	train's l1: 3.16084	test's l1: 4.48298
[390]	train's l1: 3.1529	test's l1: 4.478
[400]	train's l1: 3.14688	test's l1: 4.47426
[410]	train's l1: 3.14128	test's l1: 4.47004
[420]	train's l1: 3.12563	test's l1: 4.46509
[430]	train's l1: 3.12057	test's l1: 4.46204
[440]	train's l1: 2.96515	test's l1: 4.36872
[450]	train's l1: 2.95803	test's l1: 4.36581
[460]	train's l1: 2.94492	test's l1: 4.36569
[470]	train's l1: 2.93964	test's l1: 4.36516
[480]	train's l1: 2.93586	test's l1: 4.36417
[490]	train's l1: 2.93469	test's l1: 4.36399
[500]	train's l1: 2.93078	test's l1: 4.36516
[510]	train's l1: 2.89793	test's l1: 4.34526
[520]	train's l1: 2.89672	test's l1: 4.34454
[530]	train's l1: 2.88884	test's l1: 4.33487
[540]	train's l1: 2.88285	test's l1: 4.32817
[550]	train's l1: 2.8796	test's l1: 4.32623
[560]	train's l1: 2.87711	test's l1: 4.32573
[570]	train's l1: 2.87593	test's l1: 4.32521
[580]	train's l1: 2.83748	test's l1: 4.31918
[590]	train's l1: 2.83648	test's l1: 4.31897
[600]	train's l1: 2.83625	test's l1: 4.31898
[610]	train's l1: 2.81385	test's l1: 4.28968
[620]	train's l1: 2.80015	test's l1: 4.29301
[630]	train's l1: 2.72862	test's l1: 4.27529
[640]	train's l1: 2.65509	test's l1: 4.22818
[650]	train's l1: 2.65065	test's l1: 4.22796
[660]	train's l1: 2.64805	test's l1: 4.22816
[670]	train's l1: 2.64697	test's l1: 4.22789
[680]	train's l1: 2.64163	test's l1: 4.2264
[690]	train's l1: 2.60723	test's l1: 4.19892
[700]	train's l1: 2.60478	test's l1: 4.20008
[710]	train's l1: 2.52972	test's l1: 4.13031
[720]	train's l1: 2.49605	test's l1: 4.10039
[730]	train's l1: 2.49451	test's l1: 4.09906
[740]	train's l1: 2.49363	test's l1: 4.09858
[750]	train's l1: 2.48979	test's l1: 4.09001
[760]	train's l1: 2.48774	test's l1: 4.09039
[770]	train's l1: 2.48704	test's l1: 4.09081
[780]	train's l1: 2.48274	test's l1: 4.09089
[790]	train's l1: 2.48088	test's l1: 4.09254
[800]	train's l1: 2.48002	test's l1: 4.09266
[810]	train's l1: 2.47657	test's l1: 4.09412
[820]	train's l1: 2.47548	test's l1: 4.09366
[830]	train's l1: 2.47423	test's l1: 4.09389
[840]	train's l1: 2.47234	test's l1: 4.09291
Early stopping, best iteration is:
[748]	train's l1: 2.4901	test's l1: 4.08992
Starting for w150_False with mul=8
150: 52m30sec done
150: 52m40sec done
150: 52m50sec done
150: 53m0sec done
150: 53m10sec done
150: 53m20sec done
150: 53m30sec done
150: 53m40sec done
150: 53m50sec done
150: 54m0sec done
150: 54m10sec done
150: 54m20sec done
150: 54m30sec done
150: 54m40sec done
150: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.204336 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1520400, number of used features: 247
[LightGBM] [Info] Start training from score 71.900002
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4009	test's l1: 61.3761
[20]	train's l1: 39.9056	test's l1: 39.9698
[30]	train's l1: 26.4633	test's l1: 26.5632
[40]	train's l1: 18.3948	test's l1: 18.7454
[50]	train's l1: 11.132	test's l1: 11.496
[60]	train's l1: 7.39586	test's l1: 7.76318
[70]	train's l1: 5.6935	test's l1: 6.46754
[80]	train's l1: 5.28203	test's l1: 6.04837
[90]	train's l1: 4.71061	test's l1: 5.56465
[100]	train's l1: 4.61643	test's l1: 5.45521
[110]	train's l1: 4.37381	test's l1: 5.30341
[120]	train's l1: 4.20891	test's l1: 5.13772
[130]	train's l1: 4.20029	test's l1: 5.13356
[140]	train's l1: 4.17036	test's l1: 5.1031
[150]	train's l1: 4.06012	test's l1: 4.96248
[160]	train's l1: 3.94313	test's l1: 4.86285
[170]	train's l1: 3.9402	test's l1: 4.86198
[180]	train's l1: 3.72806	test's l1: 4.62434
[190]	train's l1: 3.60234	test's l1: 4.489
[200]	train's l1: 3.51128	test's l1: 4.41501
[210]	train's l1: 3.44083	test's l1: 4.3768
[220]	train's l1: 3.38666	test's l1: 4.32981
[230]	train's l1: 3.38023	test's l1: 4.32863
[240]	train's l1: 3.3765	test's l1: 4.32528
[250]	train's l1: 3.33507	test's l1: 4.26097
[260]	train's l1: 3.21445	test's l1: 4.16627
[270]	train's l1: 3.21184	test's l1: 4.16549
[280]	train's l1: 3.1426	test's l1: 4.13748
[290]	train's l1: 3.11987	test's l1: 4.13682
[300]	train's l1: 3.11894	test's l1: 4.13708
[310]	train's l1: 3.06911	test's l1: 4.0938
[320]	train's l1: 3.06139	test's l1: 4.0895
[330]	train's l1: 2.94191	test's l1: 4.01687
[340]	train's l1: 2.93882	test's l1: 4.01689
[350]	train's l1: 2.91989	test's l1: 4.01658
[360]	train's l1: 2.90445	test's l1: 4.00858
[370]	train's l1: 2.89999	test's l1: 4.00691
[380]	train's l1: 2.89892	test's l1: 4.00664
[390]	train's l1: 2.89845	test's l1: 4.00649
[400]	train's l1: 2.87573	test's l1: 3.98909
[410]	train's l1: 2.86891	test's l1: 3.98599
[420]	train's l1: 2.86637	test's l1: 3.98455
[430]	train's l1: 2.85135	test's l1: 3.97743
[440]	train's l1: 2.83688	test's l1: 3.9721
[450]	train's l1: 2.83343	test's l1: 3.97163
[460]	train's l1: 2.82463	test's l1: 3.96399
[470]	train's l1: 2.82221	test's l1: 3.96313
[480]	train's l1: 2.82032	test's l1: 3.96239
[490]	train's l1: 2.81626	test's l1: 3.96492
[500]	train's l1: 2.80552	test's l1: 3.95019
[510]	train's l1: 2.78843	test's l1: 3.9321
[520]	train's l1: 2.77	test's l1: 3.91302
[530]	train's l1: 2.76288	test's l1: 3.90701
[540]	train's l1: 2.75945	test's l1: 3.90404
[550]	train's l1: 2.75481	test's l1: 3.90234
[560]	train's l1: 2.74706	test's l1: 3.90198
[570]	train's l1: 2.74457	test's l1: 3.9008
[580]	train's l1: 2.73958	test's l1: 3.89374
[590]	train's l1: 2.72597	test's l1: 3.88879
[600]	train's l1: 2.7183	test's l1: 3.8866
[610]	train's l1: 2.71365	test's l1: 3.88383
[620]	train's l1: 2.69863	test's l1: 3.87558
[630]	train's l1: 2.68729	test's l1: 3.86981
[640]	train's l1: 2.60787	test's l1: 3.8063
[650]	train's l1: 2.59906	test's l1: 3.79597
[660]	train's l1: 2.59659	test's l1: 3.79546
[670]	train's l1: 2.59016	test's l1: 3.79547
[680]	train's l1: 2.54415	test's l1: 3.75598
[690]	train's l1: 2.54104	test's l1: 3.75633
[700]	train's l1: 2.5063	test's l1: 3.72672
[710]	train's l1: 2.4984	test's l1: 3.71566
[720]	train's l1: 2.48094	test's l1: 3.71394
[730]	train's l1: 2.47854	test's l1: 3.71385
[740]	train's l1: 2.47613	test's l1: 3.71297
[750]	train's l1: 2.45073	test's l1: 3.69891
[760]	train's l1: 2.44366	test's l1: 3.69561
[770]	train's l1: 2.4422	test's l1: 3.69501
[780]	train's l1: 2.42431	test's l1: 3.67206
[790]	train's l1: 2.42319	test's l1: 3.67077
[800]	train's l1: 2.42125	test's l1: 3.66623
[810]	train's l1: 2.4189	test's l1: 3.66465
[820]	train's l1: 2.41686	test's l1: 3.6643
[830]	train's l1: 2.41482	test's l1: 3.66284
[840]	train's l1: 2.4129	test's l1: 3.65875
[850]	train's l1: 2.41056	test's l1: 3.65599
[860]	train's l1: 2.39279	test's l1: 3.61377
[870]	train's l1: 2.39127	test's l1: 3.61341
[880]	train's l1: 2.38959	test's l1: 3.61179
[890]	train's l1: 2.38613	test's l1: 3.61171
[900]	train's l1: 2.38499	test's l1: 3.61155
[910]	train's l1: 2.38392	test's l1: 3.61121
[920]	train's l1: 2.38211	test's l1: 3.61059
[930]	train's l1: 2.38102	test's l1: 3.60933
[940]	train's l1: 2.38051	test's l1: 3.60928
[950]	train's l1: 2.37992	test's l1: 3.60884
[960]	train's l1: 2.37877	test's l1: 3.60895
[970]	train's l1: 2.37766	test's l1: 3.60829
[980]	train's l1: 2.37316	test's l1: 3.60495
[990]	train's l1: 2.37269	test's l1: 3.60476
[1000]	train's l1: 2.37235	test's l1: 3.60466
Did not meet early stopping. Best iteration is:
[998]	train's l1: 2.37238	test's l1: 3.60466
Starting for w100_False with mul=8
100: 53m20sec done
100: 53m30sec done
100: 53m40sec done
100: 53m50sec done
100: 54m0sec done
100: 54m10sec done
100: 54m20sec done
100: 54m30sec done
100: 54m40sec done
100: 54m50sec done
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.882336 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1940400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4808	test's l1: 61.4143
[20]	train's l1: 40.0032	test's l1: 39.9912
[30]	train's l1: 26.5456	test's l1: 26.5771
[40]	train's l1: 18.3029	test's l1: 18.5753
[50]	train's l1: 11.2086	test's l1: 11.2291
[60]	train's l1: 6.90479	test's l1: 7.03215
[70]	train's l1: 5.24646	test's l1: 5.71328
[80]	train's l1: 4.31142	test's l1: 4.91735
[90]	train's l1: 3.86477	test's l1: 4.34755
[100]	train's l1: 3.78668	test's l1: 4.26651
[110]	train's l1: 3.73726	test's l1: 4.23054
[120]	train's l1: 3.72612	test's l1: 4.21899
[130]	train's l1: 3.67209	test's l1: 4.17162
[140]	train's l1: 3.63699	test's l1: 4.1454
[150]	train's l1: 3.5915	test's l1: 4.10929
[160]	train's l1: 3.54479	test's l1: 4.04385
[170]	train's l1: 3.5132	test's l1: 4.02247
[180]	train's l1: 3.4762	test's l1: 4.00046
[190]	train's l1: 3.45285	test's l1: 3.97542
[200]	train's l1: 3.42933	test's l1: 3.95941
[210]	train's l1: 3.37514	test's l1: 3.90591
[220]	train's l1: 3.26961	test's l1: 3.83424
[230]	train's l1: 3.24248	test's l1: 3.80141
[240]	train's l1: 3.20502	test's l1: 3.75161
[250]	train's l1: 3.10417	test's l1: 3.6822
[260]	train's l1: 3.10187	test's l1: 3.68112
[270]	train's l1: 3.01876	test's l1: 3.60764
[280]	train's l1: 3.01449	test's l1: 3.60592
[290]	train's l1: 3.00471	test's l1: 3.60338
[300]	train's l1: 2.9898	test's l1: 3.59416
[310]	train's l1: 2.97719	test's l1: 3.58694
[320]	train's l1: 2.96363	test's l1: 3.58209
[330]	train's l1: 2.94826	test's l1: 3.56943
[340]	train's l1: 2.94453	test's l1: 3.56833
[350]	train's l1: 2.91726	test's l1: 3.54784
[360]	train's l1: 2.90432	test's l1: 3.53819
[370]	train's l1: 2.86549	test's l1: 3.49191
[380]	train's l1: 2.86017	test's l1: 3.48958
[390]	train's l1: 2.85553	test's l1: 3.48594
[400]	train's l1: 2.77821	test's l1: 3.40305
[410]	train's l1: 2.72698	test's l1: 3.35476
[420]	train's l1: 2.72643	test's l1: 3.3544
[430]	train's l1: 2.72436	test's l1: 3.35507
[440]	train's l1: 2.7187	test's l1: 3.35172
[450]	train's l1: 2.69944	test's l1: 3.34472
[460]	train's l1: 2.69035	test's l1: 3.3396
[470]	train's l1: 2.68187	test's l1: 3.33299
[480]	train's l1: 2.67971	test's l1: 3.33245
[490]	train's l1: 2.65894	test's l1: 3.31559
[500]	train's l1: 2.65391	test's l1: 3.31551
[510]	train's l1: 2.64955	test's l1: 3.31539
[520]	train's l1: 2.62184	test's l1: 3.29554
[530]	train's l1: 2.62107	test's l1: 3.29521
[540]	train's l1: 2.60981	test's l1: 3.29119
[550]	train's l1: 2.60126	test's l1: 3.28982
[560]	train's l1: 2.59891	test's l1: 3.28894
[570]	train's l1: 2.54386	test's l1: 3.22811
[580]	train's l1: 2.52633	test's l1: 3.22014
[590]	train's l1: 2.51856	test's l1: 3.21877
[600]	train's l1: 2.5091	test's l1: 3.21382
[610]	train's l1: 2.5073	test's l1: 3.2131
[620]	train's l1: 2.5041	test's l1: 3.20643
[630]	train's l1: 2.50212	test's l1: 3.20628
[640]	train's l1: 2.50079	test's l1: 3.20535
[650]	train's l1: 2.4918	test's l1: 3.19785
[660]	train's l1: 2.48301	test's l1: 3.19451
[670]	train's l1: 2.47131	test's l1: 3.1859
[680]	train's l1: 2.46887	test's l1: 3.18607
[690]	train's l1: 2.43897	test's l1: 3.1676
[700]	train's l1: 2.43807	test's l1: 3.1681
[710]	train's l1: 2.43655	test's l1: 3.16686
[720]	train's l1: 2.41788	test's l1: 3.15778
[730]	train's l1: 2.40582	test's l1: 3.151
[740]	train's l1: 2.39631	test's l1: 3.14502
[750]	train's l1: 2.37323	test's l1: 3.13516
[760]	train's l1: 2.36569	test's l1: 3.12761
[770]	train's l1: 2.34618	test's l1: 3.11228
[780]	train's l1: 2.32355	test's l1: 3.07625
[790]	train's l1: 2.30972	test's l1: 3.07179
[800]	train's l1: 2.3044	test's l1: 3.0685
[810]	train's l1: 2.30087	test's l1: 3.06739
[820]	train's l1: 2.27131	test's l1: 3.05638
[830]	train's l1: 2.2682	test's l1: 3.05513
[840]	train's l1: 2.26759	test's l1: 3.05518
[850]	train's l1: 2.26726	test's l1: 3.055
[860]	train's l1: 2.26661	test's l1: 3.05467
[870]	train's l1: 2.26153	test's l1: 3.05283
[880]	train's l1: 2.22714	test's l1: 3.02972
[890]	train's l1: 2.16608	test's l1: 2.97968
[900]	train's l1: 2.16455	test's l1: 2.97875
[910]	train's l1: 2.1631	test's l1: 2.97815
[920]	train's l1: 2.15912	test's l1: 2.97635
[930]	train's l1: 2.14902	test's l1: 2.97629
[940]	train's l1: 2.12194	test's l1: 2.96328
[950]	train's l1: 2.11227	test's l1: 2.95783
[960]	train's l1: 2.1102	test's l1: 2.95604
[970]	train's l1: 2.11001	test's l1: 2.95618
[980]	train's l1: 2.1062	test's l1: 2.95389
[990]	train's l1: 2.10527	test's l1: 2.95338
[1000]	train's l1: 2.0988	test's l1: 2.95111
Did not meet early stopping. Best iteration is:
[995]	train's l1: 2.10167	test's l1: 2.9509
Starting for w50_False with mul=8
50: 54m10sec done
50: 54m20sec done
50: 54m30sec done
50: 54m40sec done
50: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.275663 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 2360400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.3786	test's l1: 61.3189
[20]	train's l1: 39.869	test's l1: 39.9162
[30]	train's l1: 26.423	test's l1: 26.5066
[40]	train's l1: 18.7034	test's l1: 18.9926
[50]	train's l1: 11.3785	test's l1: 11.534
[60]	train's l1: 8.84318	test's l1: 9.00175
[70]	train's l1: 7.73572	test's l1: 7.98206
[80]	train's l1: 6.3717	test's l1: 7.10809
[90]	train's l1: 5.55457	test's l1: 6.46774
[100]	train's l1: 4.08859	test's l1: 5.32028
[110]	train's l1: 3.74116	test's l1: 5.0155
[120]	train's l1: 3.64939	test's l1: 4.91644
[130]	train's l1: 3.5471	test's l1: 4.7981
[140]	train's l1: 3.41387	test's l1: 4.67146
[150]	train's l1: 3.36417	test's l1: 4.62671
[160]	train's l1: 3.30739	test's l1: 4.56141
[170]	train's l1: 3.28119	test's l1: 4.54094
[180]	train's l1: 3.26583	test's l1: 4.52828
[190]	train's l1: 3.22826	test's l1: 4.50853
[200]	train's l1: 3.2278	test's l1: 4.50844
[210]	train's l1: 3.22407	test's l1: 4.50476
[220]	train's l1: 3.2127	test's l1: 4.49475
[230]	train's l1: 3.16078	test's l1: 4.41239
[240]	train's l1: 3.15705	test's l1: 4.41137
[250]	train's l1: 3.14341	test's l1: 4.39472
[260]	train's l1: 3.12176	test's l1: 4.38099
[270]	train's l1: 3.12066	test's l1: 4.38054
[280]	train's l1: 3.04938	test's l1: 4.29187
[290]	train's l1: 3.04641	test's l1: 4.29013
[300]	train's l1: 3.04415	test's l1: 4.28964
[310]	train's l1: 2.98259	test's l1: 4.23717
[320]	train's l1: 2.9747	test's l1: 4.23592
[330]	train's l1: 2.92519	test's l1: 4.1415
[340]	train's l1: 2.91333	test's l1: 4.1342
[350]	train's l1: 2.78786	test's l1: 4.0452
[360]	train's l1: 2.78396	test's l1: 4.04456
[370]	train's l1: 2.77941	test's l1: 4.04065
[380]	train's l1: 2.76529	test's l1: 4.02644
[390]	train's l1: 2.61657	test's l1: 3.85101
[400]	train's l1: 2.52856	test's l1: 3.73083
[410]	train's l1: 2.50294	test's l1: 3.70949
[420]	train's l1: 2.49731	test's l1: 3.70432
[430]	train's l1: 2.49625	test's l1: 3.70395
[440]	train's l1: 2.49169	test's l1: 3.70033
[450]	train's l1: 2.49096	test's l1: 3.69978
[460]	train's l1: 2.48995	test's l1: 3.69936
[470]	train's l1: 2.48816	test's l1: 3.6984
[480]	train's l1: 2.47008	test's l1: 3.68356
[490]	train's l1: 2.46902	test's l1: 3.68319
[500]	train's l1: 2.46587	test's l1: 3.68056
[510]	train's l1: 2.46269	test's l1: 3.67841
[520]	train's l1: 2.46019	test's l1: 3.67632
[530]	train's l1: 2.45475	test's l1: 3.67631
[540]	train's l1: 2.44986	test's l1: 3.67305
[550]	train's l1: 2.43525	test's l1: 3.66562
[560]	train's l1: 2.43262	test's l1: 3.66393
[570]	train's l1: 2.42011	test's l1: 3.6593
[580]	train's l1: 2.41898	test's l1: 3.65903
[590]	train's l1: 2.38908	test's l1: 3.64177
[600]	train's l1: 2.3633	test's l1: 3.62174
[610]	train's l1: 2.34504	test's l1: 3.61451
[620]	train's l1: 2.34461	test's l1: 3.61432
[630]	train's l1: 2.33195	test's l1: 3.60756
[640]	train's l1: 2.31067	test's l1: 3.59856
[650]	train's l1: 2.24797	test's l1: 3.54435
[660]	train's l1: 2.23042	test's l1: 3.53914
[670]	train's l1: 2.22877	test's l1: 3.53898
[680]	train's l1: 2.22753	test's l1: 3.53929
[690]	train's l1: 2.22653	test's l1: 3.5389
[700]	train's l1: 2.22524	test's l1: 3.53783
[710]	train's l1: 2.22444	test's l1: 3.53772
[720]	train's l1: 2.22419	test's l1: 3.5376
[730]	train's l1: 2.21815	test's l1: 3.53203
[740]	train's l1: 2.2154	test's l1: 3.52965
[750]	train's l1: 2.19657	test's l1: 3.50126
[760]	train's l1: 2.17786	test's l1: 3.48354
[770]	train's l1: 2.17008	test's l1: 3.47648
[780]	train's l1: 2.15255	test's l1: 3.44318
[790]	train's l1: 2.09542	test's l1: 3.39873
[800]	train's l1: 2.09492	test's l1: 3.39844
[810]	train's l1: 2.09162	test's l1: 3.39705
[820]	train's l1: 2.09081	test's l1: 3.39694
[830]	train's l1: 2.08692	test's l1: 3.3931
[840]	train's l1: 2.08526	test's l1: 3.39285
[850]	train's l1: 2.08497	test's l1: 3.39282
[860]	train's l1: 2.08436	test's l1: 3.39238
[870]	train's l1: 2.07556	test's l1: 3.38961
[880]	train's l1: 2.06456	test's l1: 3.37776
[890]	train's l1: 2.05815	test's l1: 3.37248
[900]	train's l1: 2.05333	test's l1: 3.36795
[910]	train's l1: 2.05283	test's l1: 3.36813
[920]	train's l1: 2.04935	test's l1: 3.36549
[930]	train's l1: 2.04614	test's l1: 3.36337
[940]	train's l1: 2.04534	test's l1: 3.36288
[950]	train's l1: 2.04429	test's l1: 3.36226
[960]	train's l1: 2.04093	test's l1: 3.36104
[970]	train's l1: 2.04036	test's l1: 3.36075
[980]	train's l1: 2.03986	test's l1: 3.36085
[990]	train's l1: 2.03914	test's l1: 3.36103
[1000]	train's l1: 2.0388	test's l1: 3.36098
Did not meet early stopping. Best iteration is:
[989]	train's l1: 2.03922	test's l1: 3.36054
Starting for w200_False with mul=9
200: 51m40sec done
200: 51m50sec done
200: 52m0sec done
200: 52m10sec done
200: 52m20sec done
200: 52m30sec done
200: 52m40sec done
200: 52m50sec done
200: 53m0sec done
200: 53m10sec done
200: 53m20sec done
200: 53m30sec done
200: 53m40sec done
200: 53m50sec done
200: 54m0sec done
200: 54m10sec done
200: 54m20sec done
200: 54m30sec done
200: 54m40sec done
200: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.133806 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 56187
[LightGBM] [Info] Number of data points in the train set: 1100400, number of used features: 228
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4824	test's l1: 61.4529
[20]	train's l1: 39.8706	test's l1: 39.929
[30]	train's l1: 26.5044	test's l1: 26.5898
[40]	train's l1: 18.3582	test's l1: 18.6927
[50]	train's l1: 11.2009	test's l1: 11.491
[60]	train's l1: 7.55645	test's l1: 7.97139
[70]	train's l1: 5.94725	test's l1: 6.61025
[80]	train's l1: 4.65736	test's l1: 5.47416
[90]	train's l1: 4.46596	test's l1: 5.2915
[100]	train's l1: 4.36713	test's l1: 5.24315
[110]	train's l1: 4.34084	test's l1: 5.20374
[120]	train's l1: 4.31801	test's l1: 5.1874
[130]	train's l1: 4.30868	test's l1: 5.19322
[140]	train's l1: 4.20446	test's l1: 5.06031
[150]	train's l1: 4.12905	test's l1: 4.96722
[160]	train's l1: 4.04854	test's l1: 4.89696
[170]	train's l1: 3.87884	test's l1: 4.8215
[180]	train's l1: 3.81914	test's l1: 4.80288
[190]	train's l1: 3.80887	test's l1: 4.80104
[200]	train's l1: 3.52618	test's l1: 4.61556
[210]	train's l1: 3.52025	test's l1: 4.61337
[220]	train's l1: 3.51687	test's l1: 4.61425
[230]	train's l1: 3.48022	test's l1: 4.60272
[240]	train's l1: 3.38763	test's l1: 4.56623
[250]	train's l1: 3.38489	test's l1: 4.56558
[260]	train's l1: 3.3552	test's l1: 4.54248
[270]	train's l1: 3.35019	test's l1: 4.53884
[280]	train's l1: 3.34219	test's l1: 4.54524
[290]	train's l1: 3.33749	test's l1: 4.54541
[300]	train's l1: 3.33284	test's l1: 4.54116
[310]	train's l1: 3.32326	test's l1: 4.5294
[320]	train's l1: 3.32195	test's l1: 4.52839
[330]	train's l1: 3.31081	test's l1: 4.50765
[340]	train's l1: 3.30555	test's l1: 4.50432
[350]	train's l1: 3.29273	test's l1: 4.50131
[360]	train's l1: 3.26433	test's l1: 4.48307
[370]	train's l1: 2.89869	test's l1: 4.3638
[380]	train's l1: 2.87338	test's l1: 4.3469
[390]	train's l1: 2.84767	test's l1: 4.34175
[400]	train's l1: 2.83974	test's l1: 4.32537
[410]	train's l1: 2.83917	test's l1: 4.32505
[420]	train's l1: 2.83024	test's l1: 4.31181
[430]	train's l1: 2.82751	test's l1: 4.30796
[440]	train's l1: 2.82272	test's l1: 4.30439
[450]	train's l1: 2.82222	test's l1: 4.30418
[460]	train's l1: 2.81909	test's l1: 4.30261
[470]	train's l1: 2.80313	test's l1: 4.28451
[480]	train's l1: 2.79976	test's l1: 4.28304
[490]	train's l1: 2.79819	test's l1: 4.28321
[500]	train's l1: 2.79726	test's l1: 4.28296
[510]	train's l1: 2.78766	test's l1: 4.27149
[520]	train's l1: 2.76708	test's l1: 4.27145
[530]	train's l1: 2.76561	test's l1: 4.27047
[540]	train's l1: 2.75877	test's l1: 4.26675
[550]	train's l1: 2.75409	test's l1: 4.26498
[560]	train's l1: 2.74083	test's l1: 4.26098
[570]	train's l1: 2.74003	test's l1: 4.26083
[580]	train's l1: 2.73883	test's l1: 4.26039
[590]	train's l1: 2.73725	test's l1: 4.25969
[600]	train's l1: 2.73683	test's l1: 4.25971
[610]	train's l1: 2.73458	test's l1: 4.25738
[620]	train's l1: 2.73205	test's l1: 4.25749
[630]	train's l1: 2.72931	test's l1: 4.2554
[640]	train's l1: 2.71637	test's l1: 4.24801
[650]	train's l1: 2.71376	test's l1: 4.24813
[660]	train's l1: 2.71325	test's l1: 4.24808
[670]	train's l1: 2.70229	test's l1: 4.24243
[680]	train's l1: 2.70173	test's l1: 4.24239
[690]	train's l1: 2.70041	test's l1: 4.24153
[700]	train's l1: 2.68586	test's l1: 4.23399
[710]	train's l1: 2.66283	test's l1: 4.22174
[720]	train's l1: 2.6523	test's l1: 4.21932
[730]	train's l1: 2.6405	test's l1: 4.20688
[740]	train's l1: 2.63949	test's l1: 4.20921
[750]	train's l1: 2.63908	test's l1: 4.20914
[760]	train's l1: 2.638	test's l1: 4.20836
[770]	train's l1: 2.63685	test's l1: 4.20906
[780]	train's l1: 2.63601	test's l1: 4.20878
[790]	train's l1: 2.63247	test's l1: 4.20867
[800]	train's l1: 2.61776	test's l1: 4.1976
[810]	train's l1: 2.61664	test's l1: 4.19732
[820]	train's l1: 2.61292	test's l1: 4.19544
[830]	train's l1: 2.60706	test's l1: 4.19464
[840]	train's l1: 2.58052	test's l1: 4.17583
[850]	train's l1: 2.57956	test's l1: 4.1757
[860]	train's l1: 2.57863	test's l1: 4.17524
[870]	train's l1: 2.57621	test's l1: 4.17486
[880]	train's l1: 2.556	test's l1: 4.15966
[890]	train's l1: 2.53809	test's l1: 4.13289
[900]	train's l1: 2.53654	test's l1: 4.13253
[910]	train's l1: 2.53441	test's l1: 4.13235
[920]	train's l1: 2.52914	test's l1: 4.13012
[930]	train's l1: 2.52834	test's l1: 4.12981
[940]	train's l1: 2.52281	test's l1: 4.12868
[950]	train's l1: 2.52047	test's l1: 4.13048
[960]	train's l1: 2.51649	test's l1: 4.12748
[970]	train's l1: 2.51367	test's l1: 4.12482
[980]	train's l1: 2.51255	test's l1: 4.12418
[990]	train's l1: 2.51216	test's l1: 4.12398
[1000]	train's l1: 2.51185	test's l1: 4.12396
Did not meet early stopping. Best iteration is:
[999]	train's l1: 2.51188	test's l1: 4.12396
Starting for w150_False with mul=9
150: 52m30sec done
150: 52m40sec done
150: 52m50sec done
150: 53m0sec done
150: 53m10sec done
150: 53m20sec done
150: 53m30sec done
150: 53m40sec done
150: 53m50sec done
150: 54m0sec done
150: 54m10sec done
150: 54m20sec done
150: 54m30sec done
150: 54m40sec done
150: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.203564 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1520400, number of used features: 247
[LightGBM] [Info] Start training from score 71.900002
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4013	test's l1: 61.3761
[20]	train's l1: 39.9124	test's l1: 39.976
[30]	train's l1: 26.4594	test's l1: 26.5593
[40]	train's l1: 18.3754	test's l1: 18.7299
[50]	train's l1: 11.1243	test's l1: 11.489
[60]	train's l1: 7.16101	test's l1: 7.49495
[70]	train's l1: 5.05034	test's l1: 6.03376
[80]	train's l1: 4.30899	test's l1: 5.50521
[90]	train's l1: 4.25724	test's l1: 5.46124
[100]	train's l1: 4.20427	test's l1: 5.41453
[110]	train's l1: 4.07928	test's l1: 5.3019
[120]	train's l1: 4.0528	test's l1: 5.28421
[130]	train's l1: 3.98946	test's l1: 5.21675
[140]	train's l1: 3.91822	test's l1: 5.16075
[150]	train's l1: 3.91168	test's l1: 5.15707
[160]	train's l1: 3.89825	test's l1: 5.14201
[170]	train's l1: 3.89311	test's l1: 5.13431
[180]	train's l1: 3.83007	test's l1: 5.09024
[190]	train's l1: 3.78271	test's l1: 5.07636
[200]	train's l1: 3.77944	test's l1: 5.07487
[210]	train's l1: 3.77621	test's l1: 5.07178
[220]	train's l1: 3.7704	test's l1: 5.06992
[230]	train's l1: 3.75937	test's l1: 5.06067
[240]	train's l1: 3.70873	test's l1: 5.03856
[250]	train's l1: 3.53677	test's l1: 4.91081
[260]	train's l1: 3.53268	test's l1: 4.90757
[270]	train's l1: 3.51888	test's l1: 4.89523
[280]	train's l1: 3.50973	test's l1: 4.89261
[290]	train's l1: 3.43525	test's l1: 4.86593
[300]	train's l1: 3.37538	test's l1: 4.84252
[310]	train's l1: 3.37212	test's l1: 4.84194
[320]	train's l1: 3.36811	test's l1: 4.84032
[330]	train's l1: 3.29166	test's l1: 4.76824
[340]	train's l1: 3.18703	test's l1: 4.69372
[350]	train's l1: 3.16456	test's l1: 4.67457
[360]	train's l1: 3.16244	test's l1: 4.67348
[370]	train's l1: 3.11557	test's l1: 4.65349
[380]	train's l1: 3.07427	test's l1: 4.6239
[390]	train's l1: 3.06954	test's l1: 4.62036
[400]	train's l1: 2.95646	test's l1: 4.57225
[410]	train's l1: 2.75797	test's l1: 4.4636
[420]	train's l1: 2.74967	test's l1: 4.45436
[430]	train's l1: 2.74534	test's l1: 4.45059
[440]	train's l1: 2.71429	test's l1: 4.38688
[450]	train's l1: 2.66687	test's l1: 4.34431
[460]	train's l1: 2.66386	test's l1: 4.34397
[470]	train's l1: 2.62917	test's l1: 4.33802
[480]	train's l1: 2.62578	test's l1: 4.33551
[490]	train's l1: 2.62479	test's l1: 4.33537
[500]	train's l1: 2.62257	test's l1: 4.33574
[510]	train's l1: 2.62112	test's l1: 4.33474
[520]	train's l1: 2.60966	test's l1: 4.32422
[530]	train's l1: 2.60675	test's l1: 4.324
[540]	train's l1: 2.60623	test's l1: 4.32391
[550]	train's l1: 2.55154	test's l1: 4.29748
[560]	train's l1: 2.51665	test's l1: 4.28856
[570]	train's l1: 2.44082	test's l1: 4.23711
[580]	train's l1: 2.416	test's l1: 4.2261
[590]	train's l1: 2.38603	test's l1: 4.21425
[600]	train's l1: 2.31726	test's l1: 4.15274
[610]	train's l1: 2.31479	test's l1: 4.15232
[620]	train's l1: 2.31312	test's l1: 4.15181
[630]	train's l1: 2.28052	test's l1: 4.12464
[640]	train's l1: 2.22363	test's l1: 4.10147
[650]	train's l1: 2.18331	test's l1: 4.04188
[660]	train's l1: 2.14195	test's l1: 3.99497
[670]	train's l1: 2.13913	test's l1: 3.9933
[680]	train's l1: 2.13803	test's l1: 3.99354
[690]	train's l1: 2.09818	test's l1: 3.96568
[700]	train's l1: 2.09601	test's l1: 3.96371
[710]	train's l1: 2.08846	test's l1: 3.96196
[720]	train's l1: 2.08739	test's l1: 3.9605
[730]	train's l1: 2.08609	test's l1: 3.95935
[740]	train's l1: 2.08548	test's l1: 3.95876
[750]	train's l1: 2.08428	test's l1: 3.95743
[760]	train's l1: 2.08375	test's l1: 3.95711
[770]	train's l1: 2.0832	test's l1: 3.95698
[780]	train's l1: 2.07717	test's l1: 3.95674
[790]	train's l1: 2.07187	test's l1: 3.9542
[800]	train's l1: 2.07082	test's l1: 3.95382
[810]	train's l1: 2.06908	test's l1: 3.95311
[820]	train's l1: 2.06833	test's l1: 3.95293
[830]	train's l1: 2.06637	test's l1: 3.95217
[840]	train's l1: 2.06557	test's l1: 3.95201
[850]	train's l1: 2.06526	test's l1: 3.95196
[860]	train's l1: 2.06502	test's l1: 3.95191
[870]	train's l1: 2.06476	test's l1: 3.95191
[880]	train's l1: 2.0611	test's l1: 3.9521
[890]	train's l1: 2.06045	test's l1: 3.95157
[900]	train's l1: 2.05996	test's l1: 3.95135
[910]	train's l1: 2.05932	test's l1: 3.951
[920]	train's l1: 2.05733	test's l1: 3.94886
[930]	train's l1: 2.05692	test's l1: 3.94852
[940]	train's l1: 2.05603	test's l1: 3.94854
[950]	train's l1: 2.05552	test's l1: 3.94836
[960]	train's l1: 2.05479	test's l1: 3.94785
[970]	train's l1: 2.05444	test's l1: 3.94778
[980]	train's l1: 2.05384	test's l1: 3.94765
[990]	train's l1: 2.05353	test's l1: 3.94762
[1000]	train's l1: 2.05304	test's l1: 3.94753
Did not meet early stopping. Best iteration is:
[999]	train's l1: 2.05308	test's l1: 3.94753
Starting for w100_False with mul=9
100: 53m20sec done
100: 53m30sec done
100: 53m40sec done
100: 53m50sec done
100: 54m0sec done
100: 54m10sec done
100: 54m20sec done
100: 54m30sec done
100: 54m40sec done
100: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.247896 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1940400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4894	test's l1: 61.4315
[20]	train's l1: 39.996	test's l1: 39.9865
[30]	train's l1: 26.5537	test's l1: 26.5908
[40]	train's l1: 18.2848	test's l1: 18.5599
[50]	train's l1: 11.264	test's l1: 11.2671
[60]	train's l1: 7.67991	test's l1: 7.61291
[70]	train's l1: 5.7626	test's l1: 6.12347
[80]	train's l1: 4.45379	test's l1: 5.17832
[90]	train's l1: 3.98371	test's l1: 4.74686
[100]	train's l1: 3.90726	test's l1: 4.66466
[110]	train's l1: 3.81942	test's l1: 4.60237
[120]	train's l1: 3.79005	test's l1: 4.56852
[130]	train's l1: 3.77073	test's l1: 4.56368
[140]	train's l1: 3.73186	test's l1: 4.55603
[150]	train's l1: 3.67855	test's l1: 4.51622
[160]	train's l1: 3.65452	test's l1: 4.49456
[170]	train's l1: 3.64364	test's l1: 4.48516
[180]	train's l1: 3.61823	test's l1: 4.4598
[190]	train's l1: 3.6084	test's l1: 4.4574
[200]	train's l1: 3.59051	test's l1: 4.44629
[210]	train's l1: 3.56907	test's l1: 4.42661
[220]	train's l1: 3.55218	test's l1: 4.41754
[230]	train's l1: 3.4753	test's l1: 4.39787
[240]	train's l1: 3.39061	test's l1: 4.36311
[250]	train's l1: 3.34478	test's l1: 4.36533
[260]	train's l1: 3.3407	test's l1: 4.3648
[270]	train's l1: 3.31947	test's l1: 4.37204
[280]	train's l1: 3.28697	test's l1: 4.35117
[290]	train's l1: 3.26497	test's l1: 4.33177
[300]	train's l1: 3.21332	test's l1: 4.30177
[310]	train's l1: 3.1835	test's l1: 4.30889
[320]	train's l1: 3.1749	test's l1: 4.30019
[330]	train's l1: 3.17068	test's l1: 4.2978
[340]	train's l1: 3.07262	test's l1: 4.20997
[350]	train's l1: 3.05427	test's l1: 4.19355
[360]	train's l1: 3.0486	test's l1: 4.19326
[370]	train's l1: 3.0462	test's l1: 4.19285
[380]	train's l1: 3.02882	test's l1: 4.18452
[390]	train's l1: 3.02253	test's l1: 4.18446
[400]	train's l1: 2.99635	test's l1: 4.16956
[410]	train's l1: 2.97474	test's l1: 4.13904
[420]	train's l1: 2.97363	test's l1: 4.13825
[430]	train's l1: 2.96177	test's l1: 4.13193
[440]	train's l1: 2.95969	test's l1: 4.13217
[450]	train's l1: 2.955	test's l1: 4.12346
[460]	train's l1: 2.95307	test's l1: 4.12262
[470]	train's l1: 2.95159	test's l1: 4.12156
[480]	train's l1: 2.94723	test's l1: 4.11744
[490]	train's l1: 2.94172	test's l1: 4.1145
[500]	train's l1: 2.89218	test's l1: 4.10381
[510]	train's l1: 2.88264	test's l1: 4.09715
[520]	train's l1: 2.87461	test's l1: 4.09204
[530]	train's l1: 2.86741	test's l1: 4.08784
[540]	train's l1: 2.86393	test's l1: 4.08445
[550]	train's l1: 2.69021	test's l1: 3.87093
[560]	train's l1: 2.66111	test's l1: 3.81727
[570]	train's l1: 2.65922	test's l1: 3.81629
[580]	train's l1: 2.65252	test's l1: 3.81751
[590]	train's l1: 2.64722	test's l1: 3.81879
[600]	train's l1: 2.62526	test's l1: 3.7777
[610]	train's l1: 2.58864	test's l1: 3.72497
[620]	train's l1: 2.56754	test's l1: 3.71404
[630]	train's l1: 2.56483	test's l1: 3.71405
[640]	train's l1: 2.56418	test's l1: 3.71403
[650]	train's l1: 2.51449	test's l1: 3.67505
[660]	train's l1: 2.45356	test's l1: 3.63328
[670]	train's l1: 2.42788	test's l1: 3.61654
[680]	train's l1: 2.34236	test's l1: 3.57219
[690]	train's l1: 2.33616	test's l1: 3.57493
[700]	train's l1: 2.31086	test's l1: 3.54814
[710]	train's l1: 2.30944	test's l1: 3.54754
[720]	train's l1: 2.30909	test's l1: 3.5474
[730]	train's l1: 2.30619	test's l1: 3.54655
[740]	train's l1: 2.30277	test's l1: 3.54431
[750]	train's l1: 2.29969	test's l1: 3.54334
[760]	train's l1: 2.29834	test's l1: 3.5429
[770]	train's l1: 2.2981	test's l1: 3.54278
[780]	train's l1: 2.29664	test's l1: 3.54307
[790]	train's l1: 2.27919	test's l1: 3.54142
[800]	train's l1: 2.27284	test's l1: 3.5392
[810]	train's l1: 2.271	test's l1: 3.53919
[820]	train's l1: 2.26441	test's l1: 3.53322
[830]	train's l1: 2.23141	test's l1: 3.50275
[840]	train's l1: 2.23072	test's l1: 3.50266
[850]	train's l1: 2.22866	test's l1: 3.50237
[860]	train's l1: 2.2282	test's l1: 3.50217
[870]	train's l1: 2.2252	test's l1: 3.50119
[880]	train's l1: 2.22282	test's l1: 3.50167
[890]	train's l1: 2.22227	test's l1: 3.50178
[900]	train's l1: 2.21721	test's l1: 3.49827
[910]	train's l1: 2.20474	test's l1: 3.48913
[920]	train's l1: 2.20089	test's l1: 3.48799
[930]	train's l1: 2.20022	test's l1: 3.4878
[940]	train's l1: 2.19987	test's l1: 3.48785
[950]	train's l1: 2.19926	test's l1: 3.48746
[960]	train's l1: 2.18733	test's l1: 3.46291
[970]	train's l1: 2.18683	test's l1: 3.46278
[980]	train's l1: 2.18585	test's l1: 3.46253
[990]	train's l1: 2.18544	test's l1: 3.46243
[1000]	train's l1: 2.18471	test's l1: 3.46195
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.18471	test's l1: 3.46195
Starting for w50_False with mul=9
