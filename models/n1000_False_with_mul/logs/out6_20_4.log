Getting data for 2025-03-03 00:00:00
Getting data for 2025-03-04 00:00:00
Getting data for 2025-03-05 00:00:00
Getting data for 2025-03-06 00:00:00
Getting data for 2025-03-07 00:00:00
Getting data for 2025-03-08 00:00:00
Getting data for 2025-03-09 00:00:00
Getting data for 2025-03-10 00:00:00
Getting data for 2025-03-11 00:00:00
Getting data for 2025-03-12 00:00:00
Getting data for 2025-03-13 00:00:00
Getting data for 2025-03-14 00:00:00
Getting data for 2025-03-15 00:00:00
Getting data for 2025-03-16 00:00:00
Getting data for 2025-03-17 00:00:00
Getting data for 2025-03-18 00:00:00
Getting data for 2025-03-19 00:00:00
Getting data for 2025-03-20 00:00:00
Getting data for 2025-03-21 00:00:00
Getting data for 2025-03-22 00:00:00
Getting data for 2025-03-23 00:00:00
Getting data for 2025-03-24 00:00:00
Getting data for 2025-03-25 00:00:00
Getting data for 2025-03-26 00:00:00
Getting data for 2025-03-27 00:00:00
Getting data for 2025-03-28 00:00:00
Getting data for 2025-03-29 00:00:00
Getting data for 2025-03-30 00:00:00
Getting data for 2025-03-31 00:00:00
1 - Basic features done
2 - Ratio features done
3 - Imbalance features done
4 - Rolling mean and std features done
5 - Diff features done
6 - Prev ref_prices features done
7 - Changes compared to prev imbalance features done
8 - MACD features done
252
Starting for w40_False with mul=6
40: 54m20sec done
40: 54m30sec done
40: 54m40sec done
40: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.299838 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 2444400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4702	test's l1: 61.4197
[20]	train's l1: 39.9218	test's l1: 39.939
[30]	train's l1: 26.4806	test's l1: 26.5489
[40]	train's l1: 18.8359	test's l1: 19.1308
[50]	train's l1: 11.3104	test's l1: 11.4352
[60]	train's l1: 7.8829	test's l1: 8.14433
[70]	train's l1: 6.39539	test's l1: 6.83836
[80]	train's l1: 5.86713	test's l1: 6.30054
[90]	train's l1: 4.68464	test's l1: 5.28644
[100]	train's l1: 4.05796	test's l1: 4.80314
[110]	train's l1: 4.02565	test's l1: 4.75993
[120]	train's l1: 3.81877	test's l1: 4.54238
[130]	train's l1: 3.79452	test's l1: 4.51736
[140]	train's l1: 3.59007	test's l1: 4.37551
[150]	train's l1: 3.45258	test's l1: 4.31765
[160]	train's l1: 3.43234	test's l1: 4.29643
[170]	train's l1: 3.42089	test's l1: 4.28789
[180]	train's l1: 3.42034	test's l1: 4.28775
[190]	train's l1: 3.39289	test's l1: 4.27216
[200]	train's l1: 3.32517	test's l1: 4.23606
[210]	train's l1: 3.28753	test's l1: 4.21914
[220]	train's l1: 3.26957	test's l1: 4.2085
[230]	train's l1: 3.25588	test's l1: 4.19602
[240]	train's l1: 3.24892	test's l1: 4.19587
[250]	train's l1: 3.18041	test's l1: 4.17351
[260]	train's l1: 3.14936	test's l1: 4.16926
[270]	train's l1: 3.0817	test's l1: 4.1233
[280]	train's l1: 3.07802	test's l1: 4.11909
[290]	train's l1: 3.06516	test's l1: 4.11479
[300]	train's l1: 3.0631	test's l1: 4.11421
[310]	train's l1: 3.06235	test's l1: 4.11414
[320]	train's l1: 3.01372	test's l1: 4.08561
[330]	train's l1: 2.93621	test's l1: 4.0469
[340]	train's l1: 2.6198	test's l1: 3.79938
[350]	train's l1: 2.5517	test's l1: 3.74369
[360]	train's l1: 2.51168	test's l1: 3.7141
[370]	train's l1: 2.49155	test's l1: 3.69766
[380]	train's l1: 2.36271	test's l1: 3.53924
[390]	train's l1: 2.34482	test's l1: 3.52222
[400]	train's l1: 2.34461	test's l1: 3.52207
[410]	train's l1: 2.3416	test's l1: 3.52265
[420]	train's l1: 2.33945	test's l1: 3.52111
[430]	train's l1: 2.33697	test's l1: 3.52325
[440]	train's l1: 2.33668	test's l1: 3.52319
[450]	train's l1: 2.32827	test's l1: 3.51793
[460]	train's l1: 2.32784	test's l1: 3.51805
[470]	train's l1: 2.32701	test's l1: 3.51762
[480]	train's l1: 2.31924	test's l1: 3.5028
[490]	train's l1: 2.31693	test's l1: 3.5012
[500]	train's l1: 2.29731	test's l1: 3.49073
[510]	train's l1: 2.29039	test's l1: 3.47987
[520]	train's l1: 2.28906	test's l1: 3.47889
[530]	train's l1: 2.28724	test's l1: 3.47826
[540]	train's l1: 2.28694	test's l1: 3.47822
[550]	train's l1: 2.26725	test's l1: 3.47
[560]	train's l1: 2.2343	test's l1: 3.4427
[570]	train's l1: 2.23145	test's l1: 3.44257
[580]	train's l1: 2.23097	test's l1: 3.4427
[590]	train's l1: 2.22942	test's l1: 3.44211
[600]	train's l1: 2.22881	test's l1: 3.44195
[610]	train's l1: 2.22563	test's l1: 3.4411
[620]	train's l1: 2.21611	test's l1: 3.42737
[630]	train's l1: 2.21294	test's l1: 3.4283
[640]	train's l1: 2.20863	test's l1: 3.4257
[650]	train's l1: 2.2084	test's l1: 3.42564
[660]	train's l1: 2.20673	test's l1: 3.42407
[670]	train's l1: 2.19911	test's l1: 3.40825
[680]	train's l1: 2.189	test's l1: 3.39718
[690]	train's l1: 2.18676	test's l1: 3.39498
[700]	train's l1: 2.1851	test's l1: 3.39371
[710]	train's l1: 2.18379	test's l1: 3.39297
[720]	train's l1: 2.18282	test's l1: 3.39272
[730]	train's l1: 2.18212	test's l1: 3.39252
[740]	train's l1: 2.17199	test's l1: 3.3845
[750]	train's l1: 2.1688	test's l1: 3.3826
[760]	train's l1: 2.16472	test's l1: 3.38207
[770]	train's l1: 2.15988	test's l1: 3.37846
[780]	train's l1: 2.15044	test's l1: 3.37167
[790]	train's l1: 2.14506	test's l1: 3.37138
[800]	train's l1: 2.12951	test's l1: 3.36436
[810]	train's l1: 2.12401	test's l1: 3.36039
[820]	train's l1: 2.12372	test's l1: 3.36031
[830]	train's l1: 2.12061	test's l1: 3.35989
[840]	train's l1: 2.11877	test's l1: 3.35915
[850]	train's l1: 2.11357	test's l1: 3.35403
[860]	train's l1: 2.11133	test's l1: 3.3549
[870]	train's l1: 2.11033	test's l1: 3.35452
[880]	train's l1: 2.10904	test's l1: 3.35449
[890]	train's l1: 2.068	test's l1: 3.33473
[900]	train's l1: 2.00643	test's l1: 3.30247
[910]	train's l1: 1.97767	test's l1: 3.29756
[920]	train's l1: 1.97592	test's l1: 3.29655
[930]	train's l1: 1.97575	test's l1: 3.29645
[940]	train's l1: 1.97253	test's l1: 3.29557
[950]	train's l1: 1.97113	test's l1: 3.2957
[960]	train's l1: 1.95598	test's l1: 3.29661
[970]	train's l1: 1.95542	test's l1: 3.29667
[980]	train's l1: 1.93452	test's l1: 3.2917
[990]	train's l1: 1.92661	test's l1: 3.2901
[1000]	train's l1: 1.92552	test's l1: 3.29059
Did not meet early stopping. Best iteration is:
[990]	train's l1: 1.92661	test's l1: 3.2901
