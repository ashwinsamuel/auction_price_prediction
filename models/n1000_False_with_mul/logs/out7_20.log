Getting data for 2025-03-03 00:00:00
Getting data for 2025-03-04 00:00:00
Getting data for 2025-03-05 00:00:00
Getting data for 2025-03-06 00:00:00
Getting data for 2025-03-07 00:00:00
Getting data for 2025-03-08 00:00:00
Getting data for 2025-03-09 00:00:00
Getting data for 2025-03-10 00:00:00
Getting data for 2025-03-11 00:00:00
Getting data for 2025-03-12 00:00:00
Getting data for 2025-03-13 00:00:00
Getting data for 2025-03-14 00:00:00
Getting data for 2025-03-15 00:00:00
Getting data for 2025-03-16 00:00:00
Getting data for 2025-03-17 00:00:00
Getting data for 2025-03-18 00:00:00
Getting data for 2025-03-19 00:00:00
Getting data for 2025-03-20 00:00:00
Getting data for 2025-03-21 00:00:00
Getting data for 2025-03-22 00:00:00
Getting data for 2025-03-23 00:00:00
Getting data for 2025-03-24 00:00:00
Getting data for 2025-03-25 00:00:00
Getting data for 2025-03-26 00:00:00
Getting data for 2025-03-27 00:00:00
Getting data for 2025-03-28 00:00:00
Getting data for 2025-03-29 00:00:00
Getting data for 2025-03-30 00:00:00
Getting data for 2025-03-31 00:00:00
1 - Basic features done
2 - Ratio features done
3 - Imbalance features done
4 - Rolling mean and std features done
5 - Diff features done
6 - Prev ref_prices features done
7 - Changes compared to prev imbalance features done
8 - MACD features done
252
Starting for w300_False with mul=7
Starting for w280_False with mul=7
280: 50m20sec done
280: 50m30sec done
280: 50m40sec done
280: 50m50sec done
280: 51m0sec done
280: 51m10sec done
280: 51m20sec done
280: 51m30sec done
280: 51m40sec done
280: 51m50sec done
280: 52m0sec done
280: 52m10sec done
280: 52m20sec done
280: 52m30sec done
280: 52m40sec done
280: 52m50sec done
280: 53m0sec done
280: 53m10sec done
280: 53m20sec done
280: 53m30sec done
280: 53m40sec done
280: 53m50sec done
280: 54m0sec done
280: 54m10sec done
280: 54m20sec done
280: 54m30sec done
280: 54m40sec done
280: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052300 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 51808
[LightGBM] [Info] Number of data points in the train set: 428400, number of used features: 209
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.552	test's l1: 61.5365
[20]	train's l1: 40.0482	test's l1: 40.113
[30]	train's l1: 26.6977	test's l1: 26.7908
[40]	train's l1: 19.1605	test's l1: 19.4788
[50]	train's l1: 11.6008	test's l1: 11.8203
[60]	train's l1: 7.9202	test's l1: 8.41204
[70]	train's l1: 6.96281	test's l1: 7.6595
[80]	train's l1: 6.42254	test's l1: 7.24003
[90]	train's l1: 5.56553	test's l1: 6.5209
[100]	train's l1: 4.85097	test's l1: 5.90367
[110]	train's l1: 4.59488	test's l1: 5.68308
[120]	train's l1: 4.40523	test's l1: 5.48416
[130]	train's l1: 4.31541	test's l1: 5.40765
[140]	train's l1: 4.29215	test's l1: 5.37311
[150]	train's l1: 4.12403	test's l1: 5.21249
[160]	train's l1: 4.07572	test's l1: 5.16896
[170]	train's l1: 3.98056	test's l1: 5.07419
[180]	train's l1: 3.9059	test's l1: 5.01257
[190]	train's l1: 3.90076	test's l1: 5.00869
[200]	train's l1: 3.89532	test's l1: 5.0011
[210]	train's l1: 3.88964	test's l1: 4.99948
[220]	train's l1: 3.85481	test's l1: 4.96721
[230]	train's l1: 3.80303	test's l1: 4.91708
[240]	train's l1: 3.66256	test's l1: 4.78758
[250]	train's l1: 3.65159	test's l1: 4.78335
[260]	train's l1: 3.64829	test's l1: 4.78274
[270]	train's l1: 3.60896	test's l1: 4.75708
[280]	train's l1: 3.58982	test's l1: 4.7468
[290]	train's l1: 3.52954	test's l1: 4.70082
[300]	train's l1: 3.52587	test's l1: 4.69862
[310]	train's l1: 3.51477	test's l1: 4.68944
[320]	train's l1: 3.51043	test's l1: 4.68625
[330]	train's l1: 3.4668	test's l1: 4.6504
[340]	train's l1: 3.46399	test's l1: 4.64927
[350]	train's l1: 3.46105	test's l1: 4.64832
[360]	train's l1: 3.44334	test's l1: 4.63293
[370]	train's l1: 3.39045	test's l1: 4.58813
[380]	train's l1: 3.10528	test's l1: 4.35074
[390]	train's l1: 3.01376	test's l1: 4.29136
[400]	train's l1: 2.82392	test's l1: 4.17239
[410]	train's l1: 2.81805	test's l1: 4.16972
[420]	train's l1: 2.8166	test's l1: 4.16934
[430]	train's l1: 2.8161	test's l1: 4.16926
[440]	train's l1: 2.81053	test's l1: 4.16484
[450]	train's l1: 2.80906	test's l1: 4.16378
[460]	train's l1: 2.80741	test's l1: 4.16043
[470]	train's l1: 2.76931	test's l1: 4.1308
[480]	train's l1: 2.76589	test's l1: 4.12934
[490]	train's l1: 2.75148	test's l1: 4.11005
[500]	train's l1: 2.75105	test's l1: 4.10999
[510]	train's l1: 2.71835	test's l1: 4.0859
[520]	train's l1: 2.66334	test's l1: 4.05109
[530]	train's l1: 2.65752	test's l1: 4.04843
[540]	train's l1: 2.64445	test's l1: 4.04819
[550]	train's l1: 2.59155	test's l1: 4.01987
[560]	train's l1: 2.55845	test's l1: 3.99201
[570]	train's l1: 2.55701	test's l1: 3.99198
[580]	train's l1: 2.55498	test's l1: 3.99077
[590]	train's l1: 2.55309	test's l1: 3.98993
[600]	train's l1: 2.55031	test's l1: 3.98824
[610]	train's l1: 2.54811	test's l1: 3.98775
[620]	train's l1: 2.54698	test's l1: 3.98707
[630]	train's l1: 2.54318	test's l1: 3.98344
[640]	train's l1: 2.54108	test's l1: 3.98309
[650]	train's l1: 2.47426	test's l1: 3.9334
[660]	train's l1: 2.44082	test's l1: 3.92505
[670]	train's l1: 2.43678	test's l1: 3.92404
[680]	train's l1: 2.43626	test's l1: 3.92391
[690]	train's l1: 2.43234	test's l1: 3.92091
[700]	train's l1: 2.42956	test's l1: 3.92109
[710]	train's l1: 2.42819	test's l1: 3.92067
[720]	train's l1: 2.42514	test's l1: 3.91813
[730]	train's l1: 2.42429	test's l1: 3.91783
[740]	train's l1: 2.42345	test's l1: 3.91779
[750]	train's l1: 2.42195	test's l1: 3.91806
[760]	train's l1: 2.42104	test's l1: 3.9183
[770]	train's l1: 2.42075	test's l1: 3.91835
[780]	train's l1: 2.42021	test's l1: 3.91818
[790]	train's l1: 2.41874	test's l1: 3.91839
[800]	train's l1: 2.41784	test's l1: 3.9182
[810]	train's l1: 2.4168	test's l1: 3.91823
[820]	train's l1: 2.41505	test's l1: 3.91826
[830]	train's l1: 2.41382	test's l1: 3.91807
[840]	train's l1: 2.41101	test's l1: 3.91735
[850]	train's l1: 2.39238	test's l1: 3.91707
[860]	train's l1: 2.37448	test's l1: 3.90516
[870]	train's l1: 2.37381	test's l1: 3.90538
[880]	train's l1: 2.37242	test's l1: 3.90475
[890]	train's l1: 2.37103	test's l1: 3.90349
[900]	train's l1: 2.37031	test's l1: 3.90334
[910]	train's l1: 2.36841	test's l1: 3.90217
[920]	train's l1: 2.36749	test's l1: 3.90169
[930]	train's l1: 2.36605	test's l1: 3.90059
[940]	train's l1: 2.36101	test's l1: 3.90499
[950]	train's l1: 2.36019	test's l1: 3.90462
[960]	train's l1: 2.35674	test's l1: 3.90306
[970]	train's l1: 2.35458	test's l1: 3.90185
[980]	train's l1: 2.35368	test's l1: 3.90145
[990]	train's l1: 2.35302	test's l1: 3.90135
[1000]	train's l1: 2.34868	test's l1: 3.90063
Did not meet early stopping. Best iteration is:
[937]	train's l1: 2.36424	test's l1: 3.89944
Starting for w260_False with mul=7
260: 50m40sec done
260: 50m50sec done
260: 51m0sec done
260: 51m10sec done
260: 51m20sec done
260: 51m30sec done
260: 51m40sec done
260: 51m50sec done
260: 52m0sec done
260: 52m10sec done
260: 52m20sec done
260: 52m30sec done
260: 52m40sec done
260: 52m50sec done
260: 53m0sec done
260: 53m10sec done
260: 53m20sec done
260: 53m30sec done
260: 53m40sec done
260: 53m50sec done
260: 54m0sec done
260: 54m10sec done
260: 54m20sec done
260: 54m30sec done
260: 54m40sec done
260: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054147 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 51826
[LightGBM] [Info] Number of data points in the train set: 596400, number of used features: 209
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4843	test's l1: 61.4601
[20]	train's l1: 40.0083	test's l1: 40.0675
[30]	train's l1: 26.6413	test's l1: 26.7465
[40]	train's l1: 18.4458	test's l1: 18.8133
[50]	train's l1: 11.2596	test's l1: 11.7661
[60]	train's l1: 8.90502	test's l1: 9.70684
[70]	train's l1: 6.7682	test's l1: 7.704
[80]	train's l1: 6.03045	test's l1: 6.99878
[90]	train's l1: 5.44406	test's l1: 6.51419
[100]	train's l1: 4.76696	test's l1: 6.02434
[110]	train's l1: 4.51122	test's l1: 5.8092
[120]	train's l1: 4.31917	test's l1: 5.66371
[130]	train's l1: 4.17947	test's l1: 5.54196
[140]	train's l1: 3.99239	test's l1: 5.35803
[150]	train's l1: 3.94558	test's l1: 5.31983
[160]	train's l1: 3.83306	test's l1: 5.21765
[170]	train's l1: 3.7957	test's l1: 5.18123
[180]	train's l1: 3.77921	test's l1: 5.15485
[190]	train's l1: 3.7403	test's l1: 5.1188
[200]	train's l1: 3.71533	test's l1: 5.10711
[210]	train's l1: 3.70964	test's l1: 5.10889
[220]	train's l1: 3.67852	test's l1: 5.08182
[230]	train's l1: 3.66516	test's l1: 5.07807
[240]	train's l1: 3.63674	test's l1: 5.05952
[250]	train's l1: 3.57157	test's l1: 5.00085
[260]	train's l1: 3.54241	test's l1: 4.96914
[270]	train's l1: 3.48935	test's l1: 4.93327
[280]	train's l1: 3.45242	test's l1: 4.92233
[290]	train's l1: 3.41951	test's l1: 4.90256
[300]	train's l1: 3.40563	test's l1: 4.894
[310]	train's l1: 3.40008	test's l1: 4.89202
[320]	train's l1: 3.37962	test's l1: 4.89093
[330]	train's l1: 3.27423	test's l1: 4.80512
[340]	train's l1: 3.27211	test's l1: 4.80392
[350]	train's l1: 3.20362	test's l1: 4.75531
[360]	train's l1: 3.17925	test's l1: 4.74045
[370]	train's l1: 3.1232	test's l1: 4.65891
[380]	train's l1: 3.0925	test's l1: 4.61425
[390]	train's l1: 3.04945	test's l1: 4.59915
[400]	train's l1: 3.0252	test's l1: 4.58239
[410]	train's l1: 3.02205	test's l1: 4.58326
[420]	train's l1: 3.0167	test's l1: 4.58911
[430]	train's l1: 3.01293	test's l1: 4.59305
[440]	train's l1: 3.01006	test's l1: 4.59339
[450]	train's l1: 3.00782	test's l1: 4.58968
[460]	train's l1: 2.99379	test's l1: 4.59039
[470]	train's l1: 2.98741	test's l1: 4.59277
[480]	train's l1: 2.9824	test's l1: 4.58952
[490]	train's l1: 2.97971	test's l1: 4.58806
Early stopping, best iteration is:
[399]	train's l1: 3.02576	test's l1: 4.5822
Starting for w240_False with mul=7
240: 51m0sec done
240: 51m10sec done
240: 51m20sec done
240: 51m30sec done
240: 51m40sec done
240: 51m50sec done
240: 52m0sec done
240: 52m10sec done
240: 52m20sec done
240: 52m30sec done
240: 52m40sec done
240: 52m50sec done
240: 53m0sec done
240: 53m10sec done
240: 53m20sec done
240: 53m30sec done
240: 53m40sec done
240: 53m50sec done
240: 54m0sec done
240: 54m10sec done
240: 54m20sec done
240: 54m30sec done
240: 54m40sec done
240: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095289 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 56187
[LightGBM] [Info] Number of data points in the train set: 764400, number of used features: 228
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4215	test's l1: 61.4003
[20]	train's l1: 39.8908	test's l1: 39.9857
[30]	train's l1: 26.4717	test's l1: 26.5992
[40]	train's l1: 18.3116	test's l1: 18.6951
[50]	train's l1: 11.258	test's l1: 11.6549
[60]	train's l1: 7.63429	test's l1: 8.17725
[70]	train's l1: 6.49754	test's l1: 7.27486
[80]	train's l1: 4.80681	test's l1: 5.61478
[90]	train's l1: 4.46648	test's l1: 5.22052
[100]	train's l1: 4.33913	test's l1: 5.07949
[110]	train's l1: 4.28789	test's l1: 5.02348
[120]	train's l1: 4.12111	test's l1: 4.88555
[130]	train's l1: 3.87846	test's l1: 4.73387
[140]	train's l1: 3.73719	test's l1: 4.58796
[150]	train's l1: 3.68828	test's l1: 4.5517
[160]	train's l1: 3.67914	test's l1: 4.54227
[170]	train's l1: 3.66088	test's l1: 4.51673
[180]	train's l1: 3.61818	test's l1: 4.49708
[190]	train's l1: 3.54456	test's l1: 4.43071
[200]	train's l1: 3.54109	test's l1: 4.42797
[210]	train's l1: 3.51495	test's l1: 4.40688
[220]	train's l1: 3.48596	test's l1: 4.36343
[230]	train's l1: 3.47949	test's l1: 4.36788
[240]	train's l1: 3.47066	test's l1: 4.35914
[250]	train's l1: 3.46768	test's l1: 4.35486
[260]	train's l1: 3.46508	test's l1: 4.35429
[270]	train's l1: 3.45807	test's l1: 4.34929
[280]	train's l1: 3.45203	test's l1: 4.34569
[290]	train's l1: 3.40337	test's l1: 4.28552
[300]	train's l1: 3.39444	test's l1: 4.27642
[310]	train's l1: 3.32618	test's l1: 4.20528
[320]	train's l1: 3.26051	test's l1: 4.13258
[330]	train's l1: 3.2308	test's l1: 4.11457
[340]	train's l1: 3.19545	test's l1: 4.05418
[350]	train's l1: 2.99314	test's l1: 3.90922
[360]	train's l1: 2.73153	test's l1: 3.68635
[370]	train's l1: 2.6419	test's l1: 3.60912
[380]	train's l1: 2.63585	test's l1: 3.60669
[390]	train's l1: 2.59731	test's l1: 3.58757
[400]	train's l1: 2.5805	test's l1: 3.57689
[410]	train's l1: 2.56609	test's l1: 3.57228
[420]	train's l1: 2.54782	test's l1: 3.5439
[430]	train's l1: 2.54221	test's l1: 3.54128
[440]	train's l1: 2.53959	test's l1: 3.54368
[450]	train's l1: 2.53434	test's l1: 3.54074
[460]	train's l1: 2.53247	test's l1: 3.54074
[470]	train's l1: 2.49243	test's l1: 3.51687
[480]	train's l1: 2.45178	test's l1: 3.4909
[490]	train's l1: 2.45031	test's l1: 3.49064
[500]	train's l1: 2.44687	test's l1: 3.48943
[510]	train's l1: 2.44555	test's l1: 3.48904
[520]	train's l1: 2.43445	test's l1: 3.47149
[530]	train's l1: 2.4334	test's l1: 3.47159
[540]	train's l1: 2.4327	test's l1: 3.47155
[550]	train's l1: 2.4156	test's l1: 3.45327
[560]	train's l1: 2.35071	test's l1: 3.43612
[570]	train's l1: 2.31023	test's l1: 3.43282
[580]	train's l1: 2.27961	test's l1: 3.42054
[590]	train's l1: 2.27746	test's l1: 3.41851
[600]	train's l1: 2.27656	test's l1: 3.4181
[610]	train's l1: 2.27517	test's l1: 3.4168
[620]	train's l1: 2.27401	test's l1: 3.41618
[630]	train's l1: 2.27186	test's l1: 3.41518
[640]	train's l1: 2.27126	test's l1: 3.41572
[650]	train's l1: 2.2704	test's l1: 3.41588
[660]	train's l1: 2.26776	test's l1: 3.41399
[670]	train's l1: 2.26629	test's l1: 3.4138
[680]	train's l1: 2.26558	test's l1: 3.41366
[690]	train's l1: 2.2649	test's l1: 3.41331
[700]	train's l1: 2.26324	test's l1: 3.41289
[710]	train's l1: 2.26208	test's l1: 3.4119
[720]	train's l1: 2.26059	test's l1: 3.41172
[730]	train's l1: 2.25555	test's l1: 3.41171
[740]	train's l1: 2.25373	test's l1: 3.41045
[750]	train's l1: 2.25052	test's l1: 3.4082
[760]	train's l1: 2.23977	test's l1: 3.39592
[770]	train's l1: 2.23036	test's l1: 3.40015
[780]	train's l1: 2.22427	test's l1: 3.40025
[790]	train's l1: 2.21756	test's l1: 3.39938
[800]	train's l1: 2.21363	test's l1: 3.3918
[810]	train's l1: 2.20823	test's l1: 3.39239
[820]	train's l1: 2.20442	test's l1: 3.38959
[830]	train's l1: 2.20257	test's l1: 3.38882
[840]	train's l1: 2.19646	test's l1: 3.38152
[850]	train's l1: 2.17628	test's l1: 3.36802
[860]	train's l1: 2.17512	test's l1: 3.36797
[870]	train's l1: 2.17161	test's l1: 3.36622
[880]	train's l1: 2.16709	test's l1: 3.3653
[890]	train's l1: 2.1664	test's l1: 3.36513
[900]	train's l1: 2.16244	test's l1: 3.3638
[910]	train's l1: 2.16207	test's l1: 3.36365
[920]	train's l1: 2.16134	test's l1: 3.36332
[930]	train's l1: 2.15346	test's l1: 3.36169
[940]	train's l1: 2.12899	test's l1: 3.34343
[950]	train's l1: 2.12429	test's l1: 3.34186
[960]	train's l1: 2.12251	test's l1: 3.34209
[970]	train's l1: 2.11714	test's l1: 3.34156
[980]	train's l1: 2.1113	test's l1: 3.33762
[990]	train's l1: 2.11084	test's l1: 3.33782
[1000]	train's l1: 2.10976	test's l1: 3.33789
Did not meet early stopping. Best iteration is:
[981]	train's l1: 2.11126	test's l1: 3.33759
Starting for w220_False with mul=7
220: 51m20sec done
220: 51m30sec done
220: 51m40sec done
220: 51m50sec done
220: 52m0sec done
220: 52m10sec done
220: 52m20sec done
220: 52m30sec done
220: 52m40sec done
220: 52m50sec done
220: 53m0sec done
220: 53m10sec done
220: 53m20sec done
220: 53m30sec done
220: 53m40sec done
220: 53m50sec done
220: 54m0sec done
220: 54m10sec done
220: 54m20sec done
220: 54m30sec done
220: 54m40sec done
220: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.110976 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 56187
[LightGBM] [Info] Number of data points in the train set: 932400, number of used features: 228
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4818	test's l1: 61.4405
[20]	train's l1: 39.933	test's l1: 39.9978
[30]	train's l1: 26.5282	test's l1: 26.629
[40]	train's l1: 18.3484	test's l1: 18.7267
[50]	train's l1: 11.1606	test's l1: 11.5221
[60]	train's l1: 6.90666	test's l1: 7.58913
[70]	train's l1: 6.30297	test's l1: 7.23623
[80]	train's l1: 5.44201	test's l1: 6.51458
[90]	train's l1: 4.79558	test's l1: 5.99244
[100]	train's l1: 4.58368	test's l1: 5.73131
[110]	train's l1: 4.50002	test's l1: 5.61525
[120]	train's l1: 4.25011	test's l1: 5.44068
[130]	train's l1: 4.22973	test's l1: 5.42356
[140]	train's l1: 3.92418	test's l1: 5.21473
[150]	train's l1: 3.54361	test's l1: 4.91242
[160]	train's l1: 3.39287	test's l1: 4.77936
[170]	train's l1: 3.36808	test's l1: 4.74758
[180]	train's l1: 3.31118	test's l1: 4.70779
[190]	train's l1: 3.30532	test's l1: 4.7026
[200]	train's l1: 3.30105	test's l1: 4.69931
[210]	train's l1: 3.29802	test's l1: 4.69748
[220]	train's l1: 3.28158	test's l1: 4.6807
[230]	train's l1: 3.2774	test's l1: 4.68037
[240]	train's l1: 3.23561	test's l1: 4.65467
[250]	train's l1: 3.22915	test's l1: 4.65047
[260]	train's l1: 3.10382	test's l1: 4.55072
[270]	train's l1: 3.09974	test's l1: 4.54738
[280]	train's l1: 3.04147	test's l1: 4.51391
[290]	train's l1: 3.03907	test's l1: 4.51407
[300]	train's l1: 3.03699	test's l1: 4.51301
[310]	train's l1: 3.03263	test's l1: 4.51072
[320]	train's l1: 3.00126	test's l1: 4.47053
[330]	train's l1: 2.99839	test's l1: 4.47063
[340]	train's l1: 2.99708	test's l1: 4.47037
[350]	train's l1: 2.99532	test's l1: 4.47027
[360]	train's l1: 2.81706	test's l1: 4.30726
[370]	train's l1: 2.79897	test's l1: 4.30054
[380]	train's l1: 2.76005	test's l1: 4.26832
[390]	train's l1: 2.70557	test's l1: 4.21468
[400]	train's l1: 2.69791	test's l1: 4.20635
[410]	train's l1: 2.69347	test's l1: 4.20361
[420]	train's l1: 2.68932	test's l1: 4.19709
[430]	train's l1: 2.68842	test's l1: 4.19671
[440]	train's l1: 2.62393	test's l1: 4.146
[450]	train's l1: 2.61604	test's l1: 4.1404
[460]	train's l1: 2.57409	test's l1: 4.10931
[470]	train's l1: 2.55088	test's l1: 4.0646
[480]	train's l1: 2.54351	test's l1: 4.0626
[490]	train's l1: 2.54044	test's l1: 4.0612
[500]	train's l1: 2.49498	test's l1: 4.02988
[510]	train's l1: 2.47896	test's l1: 4.01528
[520]	train's l1: 2.41418	test's l1: 3.97681
[530]	train's l1: 2.413	test's l1: 3.97563
[540]	train's l1: 2.40646	test's l1: 3.97438
[550]	train's l1: 2.38632	test's l1: 3.94578
[560]	train's l1: 2.3851	test's l1: 3.94471
[570]	train's l1: 2.38318	test's l1: 3.9443
[580]	train's l1: 2.38262	test's l1: 3.94457
[590]	train's l1: 2.36564	test's l1: 3.91474
[600]	train's l1: 2.36263	test's l1: 3.91469
[610]	train's l1: 2.35571	test's l1: 3.90865
[620]	train's l1: 2.34293	test's l1: 3.87844
[630]	train's l1: 2.34128	test's l1: 3.87517
[640]	train's l1: 2.33944	test's l1: 3.87533
[650]	train's l1: 2.33448	test's l1: 3.87441
[660]	train's l1: 2.3211	test's l1: 3.86567
[670]	train's l1: 2.31893	test's l1: 3.86567
[680]	train's l1: 2.31594	test's l1: 3.86641
[690]	train's l1: 2.31448	test's l1: 3.86433
[700]	train's l1: 2.31274	test's l1: 3.86334
[710]	train's l1: 2.30972	test's l1: 3.86131
[720]	train's l1: 2.30842	test's l1: 3.86036
[730]	train's l1: 2.30646	test's l1: 3.86069
[740]	train's l1: 2.30521	test's l1: 3.8605
[750]	train's l1: 2.30448	test's l1: 3.86033
[760]	train's l1: 2.30312	test's l1: 3.85952
[770]	train's l1: 2.27703	test's l1: 3.83788
[780]	train's l1: 2.27434	test's l1: 3.83584
[790]	train's l1: 2.27149	test's l1: 3.83357
[800]	train's l1: 2.2695	test's l1: 3.83261
[810]	train's l1: 2.26441	test's l1: 3.82981
[820]	train's l1: 2.23893	test's l1: 3.80498
[830]	train's l1: 2.23686	test's l1: 3.80489
[840]	train's l1: 2.23571	test's l1: 3.80415
[850]	train's l1: 2.23403	test's l1: 3.80409
[860]	train's l1: 2.22632	test's l1: 3.78185
[870]	train's l1: 2.2196	test's l1: 3.76268
[880]	train's l1: 2.18563	test's l1: 3.73905
[890]	train's l1: 2.18526	test's l1: 3.73891
[900]	train's l1: 2.18392	test's l1: 3.73869
[910]	train's l1: 2.17563	test's l1: 3.7361
[920]	train's l1: 2.17482	test's l1: 3.73634
[930]	train's l1: 2.17426	test's l1: 3.73576
[940]	train's l1: 2.16996	test's l1: 3.73341
[950]	train's l1: 2.16914	test's l1: 3.73336
[960]	train's l1: 2.16579	test's l1: 3.73221
[970]	train's l1: 2.16499	test's l1: 3.73256
[980]	train's l1: 2.1639	test's l1: 3.73254
[990]	train's l1: 2.16343	test's l1: 3.73233
[1000]	train's l1: 2.16153	test's l1: 3.73214
Did not meet early stopping. Best iteration is:
[954]	train's l1: 2.16686	test's l1: 3.7316
Starting for w200_False with mul=7
Starting for w180_False with mul=7
180: 52m0sec done
180: 52m10sec done
180: 52m20sec done
180: 52m30sec done
180: 52m40sec done
180: 52m50sec done
180: 53m0sec done
180: 53m10sec done
180: 53m20sec done
180: 53m30sec done
180: 53m40sec done
180: 53m50sec done
180: 54m0sec done
180: 54m10sec done
180: 54m20sec done
180: 54m30sec done
180: 54m40sec done
180: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.175708 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1268400, number of used features: 247
[LightGBM] [Info] Start training from score 71.900002
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4095	test's l1: 61.3859
[20]	train's l1: 39.8356	test's l1: 39.924
[30]	train's l1: 26.4709	test's l1: 26.5972
[40]	train's l1: 18.8737	test's l1: 19.2267
[50]	train's l1: 11.4251	test's l1: 11.6299
[60]	train's l1: 7.76439	test's l1: 8.05184
[70]	train's l1: 5.91737	test's l1: 6.62872
[80]	train's l1: 5.11274	test's l1: 5.90914
[90]	train's l1: 4.37785	test's l1: 5.23353
[100]	train's l1: 4.13009	test's l1: 5.04462
[110]	train's l1: 4.05968	test's l1: 4.95418
[120]	train's l1: 4.02907	test's l1: 4.92566
[130]	train's l1: 3.99582	test's l1: 4.89083
[140]	train's l1: 3.88914	test's l1: 4.7839
[150]	train's l1: 3.85403	test's l1: 4.75988
[160]	train's l1: 3.79124	test's l1: 4.72895
[170]	train's l1: 3.77813	test's l1: 4.71997
[180]	train's l1: 3.77458	test's l1: 4.71801
[190]	train's l1: 3.77143	test's l1: 4.71634
[200]	train's l1: 3.76868	test's l1: 4.71554
[210]	train's l1: 3.60101	test's l1: 4.60999
[220]	train's l1: 3.36604	test's l1: 4.50319
[230]	train's l1: 3.35692	test's l1: 4.50101
[240]	train's l1: 3.32648	test's l1: 4.4895
[250]	train's l1: 3.32221	test's l1: 4.48934
[260]	train's l1: 3.31067	test's l1: 4.47849
[270]	train's l1: 3.30619	test's l1: 4.47431
[280]	train's l1: 3.29144	test's l1: 4.45684
[290]	train's l1: 3.27278	test's l1: 4.45908
[300]	train's l1: 3.17525	test's l1: 4.37472
[310]	train's l1: 3.08304	test's l1: 4.26773
[320]	train's l1: 3.07342	test's l1: 4.26259
[330]	train's l1: 3.07278	test's l1: 4.26267
[340]	train's l1: 3.05106	test's l1: 4.24207
[350]	train's l1: 3.02882	test's l1: 4.22612
[360]	train's l1: 2.9613	test's l1: 4.16495
[370]	train's l1: 2.9207	test's l1: 4.10769
[380]	train's l1: 2.91316	test's l1: 4.0997
[390]	train's l1: 2.8866	test's l1: 4.07994
[400]	train's l1: 2.88522	test's l1: 4.07957
[410]	train's l1: 2.88226	test's l1: 4.07751
[420]	train's l1: 2.8815	test's l1: 4.07745
[430]	train's l1: 2.87579	test's l1: 4.06948
[440]	train's l1: 2.87273	test's l1: 4.06934
[450]	train's l1: 2.85135	test's l1: 4.0426
[460]	train's l1: 2.84954	test's l1: 4.04218
[470]	train's l1: 2.84753	test's l1: 4.04169
[480]	train's l1: 2.84679	test's l1: 4.04205
[490]	train's l1: 2.83667	test's l1: 4.03535
[500]	train's l1: 2.83291	test's l1: 4.03176
[510]	train's l1: 2.82108	test's l1: 4.02826
[520]	train's l1: 2.79316	test's l1: 4.00693
[530]	train's l1: 2.63841	test's l1: 3.8737
[540]	train's l1: 2.6374	test's l1: 3.87334
[550]	train's l1: 2.63205	test's l1: 3.86089
[560]	train's l1: 2.61623	test's l1: 3.8496
[570]	train's l1: 2.61489	test's l1: 3.84856
[580]	train's l1: 2.6105	test's l1: 3.8459
[590]	train's l1: 2.6079	test's l1: 3.84463
[600]	train's l1: 2.6067	test's l1: 3.84441
[610]	train's l1: 2.59986	test's l1: 3.83024
[620]	train's l1: 2.59564	test's l1: 3.82993
[630]	train's l1: 2.58938	test's l1: 3.82519
[640]	train's l1: 2.58693	test's l1: 3.82373
[650]	train's l1: 2.57059	test's l1: 3.80545
[660]	train's l1: 2.53221	test's l1: 3.76831
[670]	train's l1: 2.53161	test's l1: 3.76797
[680]	train's l1: 2.53073	test's l1: 3.76727
[690]	train's l1: 2.52066	test's l1: 3.76609
[700]	train's l1: 2.51762	test's l1: 3.76501
[710]	train's l1: 2.51386	test's l1: 3.7556
[720]	train's l1: 2.49281	test's l1: 3.74123
[730]	train's l1: 2.47884	test's l1: 3.72498
[740]	train's l1: 2.47603	test's l1: 3.72423
[750]	train's l1: 2.4704	test's l1: 3.71864
[760]	train's l1: 2.46834	test's l1: 3.71699
[770]	train's l1: 2.45115	test's l1: 3.70123
[780]	train's l1: 2.44839	test's l1: 3.70121
[790]	train's l1: 2.42145	test's l1: 3.68222
[800]	train's l1: 2.41502	test's l1: 3.67603
[810]	train's l1: 2.40897	test's l1: 3.67314
[820]	train's l1: 2.40668	test's l1: 3.67156
[830]	train's l1: 2.405	test's l1: 3.67121
[840]	train's l1: 2.39655	test's l1: 3.66496
[850]	train's l1: 2.39584	test's l1: 3.66447
[860]	train's l1: 2.3955	test's l1: 3.66458
[870]	train's l1: 2.335	test's l1: 3.62602
[880]	train's l1: 2.30216	test's l1: 3.6011
[890]	train's l1: 2.28962	test's l1: 3.59989
[900]	train's l1: 2.28933	test's l1: 3.59971
[910]	train's l1: 2.28669	test's l1: 3.597
[920]	train's l1: 2.28595	test's l1: 3.59682
[930]	train's l1: 2.28375	test's l1: 3.5957
[940]	train's l1: 2.28323	test's l1: 3.59592
[950]	train's l1: 2.27754	test's l1: 3.60035
[960]	train's l1: 2.27656	test's l1: 3.60035
[970]	train's l1: 2.27481	test's l1: 3.5985
[980]	train's l1: 2.27318	test's l1: 3.59785
[990]	train's l1: 2.27144	test's l1: 3.59367
[1000]	train's l1: 2.26574	test's l1: 3.59399
Did not meet early stopping. Best iteration is:
[996]	train's l1: 2.26716	test's l1: 3.59339
Starting for w160_False with mul=7
160: 52m20sec done
160: 52m30sec done
160: 52m40sec done
160: 52m50sec done
160: 53m0sec done
160: 53m10sec done
160: 53m20sec done
160: 53m30sec done
160: 53m40sec done
160: 53m50sec done
160: 54m0sec done
160: 54m10sec done
160: 54m20sec done
160: 54m30sec done
160: 54m40sec done
160: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.185509 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1436400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4247	test's l1: 61.3552
[20]	train's l1: 39.8827	test's l1: 39.9061
[30]	train's l1: 26.4913	test's l1: 26.5775
[40]	train's l1: 18.9393	test's l1: 19.2643
[50]	train's l1: 11.3312	test's l1: 11.5781
[60]	train's l1: 7.26134	test's l1: 7.72569
[70]	train's l1: 5.72251	test's l1: 6.34026
[80]	train's l1: 4.78983	test's l1: 5.51908
[90]	train's l1: 4.18378	test's l1: 5.0176
[100]	train's l1: 3.9605	test's l1: 4.83626
[110]	train's l1: 3.86933	test's l1: 4.74709
[120]	train's l1: 3.81928	test's l1: 4.72077
[130]	train's l1: 3.79398	test's l1: 4.70593
[140]	train's l1: 3.67596	test's l1: 4.5971
[150]	train's l1: 3.67485	test's l1: 4.59672
[160]	train's l1: 3.60224	test's l1: 4.5344
[170]	train's l1: 3.49176	test's l1: 4.43101
[180]	train's l1: 3.42034	test's l1: 4.36353
[190]	train's l1: 3.37016	test's l1: 4.34273
[200]	train's l1: 3.3636	test's l1: 4.34136
[210]	train's l1: 3.35544	test's l1: 4.33886
[220]	train's l1: 3.3333	test's l1: 4.32991
[230]	train's l1: 3.3285	test's l1: 4.32657
[240]	train's l1: 3.27645	test's l1: 4.29897
[250]	train's l1: 3.27151	test's l1: 4.29627
[260]	train's l1: 3.25993	test's l1: 4.28424
[270]	train's l1: 3.24622	test's l1: 4.27454
[280]	train's l1: 3.22157	test's l1: 4.26046
[290]	train's l1: 3.18726	test's l1: 4.23156
[300]	train's l1: 3.14353	test's l1: 4.20228
[310]	train's l1: 3.13713	test's l1: 4.19906
[320]	train's l1: 3.13397	test's l1: 4.19645
[330]	train's l1: 3.12922	test's l1: 4.19331
[340]	train's l1: 3.0908	test's l1: 4.13471
[350]	train's l1: 3.07372	test's l1: 4.12192
[360]	train's l1: 3.0682	test's l1: 4.12167
[370]	train's l1: 3.05271	test's l1: 4.11748
[380]	train's l1: 3.03947	test's l1: 4.11243
[390]	train's l1: 3.03571	test's l1: 4.11129
[400]	train's l1: 2.97613	test's l1: 4.06756
[410]	train's l1: 2.88162	test's l1: 4.0123
[420]	train's l1: 2.63595	test's l1: 3.80618
[430]	train's l1: 2.61812	test's l1: 3.79513
[440]	train's l1: 2.6148	test's l1: 3.79383
[450]	train's l1: 2.61251	test's l1: 3.79395
[460]	train's l1: 2.61063	test's l1: 3.7937
[470]	train's l1: 2.60913	test's l1: 3.79356
[480]	train's l1: 2.57502	test's l1: 3.79509
[490]	train's l1: 2.57074	test's l1: 3.79097
[500]	train's l1: 2.5395	test's l1: 3.77079
[510]	train's l1: 2.53204	test's l1: 3.76996
[520]	train's l1: 2.51779	test's l1: 3.76564
[530]	train's l1: 2.51037	test's l1: 3.76588
[540]	train's l1: 2.50451	test's l1: 3.76048
[550]	train's l1: 2.48626	test's l1: 3.76305
[560]	train's l1: 2.4822	test's l1: 3.76146
[570]	train's l1: 2.47917	test's l1: 3.76036
[580]	train's l1: 2.47154	test's l1: 3.75884
[590]	train's l1: 2.46374	test's l1: 3.76191
[600]	train's l1: 2.45687	test's l1: 3.76407
[610]	train's l1: 2.4153	test's l1: 3.74239
[620]	train's l1: 2.39023	test's l1: 3.72746
[630]	train's l1: 2.38878	test's l1: 3.72785
[640]	train's l1: 2.38089	test's l1: 3.72326
[650]	train's l1: 2.37903	test's l1: 3.72263
[660]	train's l1: 2.37567	test's l1: 3.72393
[670]	train's l1: 2.37483	test's l1: 3.72394
[680]	train's l1: 2.37347	test's l1: 3.72469
[690]	train's l1: 2.37235	test's l1: 3.72445
[700]	train's l1: 2.36687	test's l1: 3.72407
[710]	train's l1: 2.36601	test's l1: 3.72369
[720]	train's l1: 2.36374	test's l1: 3.72342
[730]	train's l1: 2.35604	test's l1: 3.72076
[740]	train's l1: 2.35505	test's l1: 3.71986
[750]	train's l1: 2.3546	test's l1: 3.71978
[760]	train's l1: 2.35097	test's l1: 3.71842
[770]	train's l1: 2.21754	test's l1: 3.6678
[780]	train's l1: 2.10312	test's l1: 3.57276
[790]	train's l1: 2.06699	test's l1: 3.54224
[800]	train's l1: 2.06409	test's l1: 3.54245
[810]	train's l1: 2.06185	test's l1: 3.54204
[820]	train's l1: 2.06146	test's l1: 3.54198
[830]	train's l1: 2.06105	test's l1: 3.54193
[840]	train's l1: 2.05954	test's l1: 3.54166
[850]	train's l1: 2.05915	test's l1: 3.5415
[860]	train's l1: 2.05869	test's l1: 3.54128
[870]	train's l1: 2.05819	test's l1: 3.5413
[880]	train's l1: 2.05045	test's l1: 3.53757
[890]	train's l1: 2.04793	test's l1: 3.53682
[900]	train's l1: 2.04224	test's l1: 3.53644
[910]	train's l1: 2.0355	test's l1: 3.53189
[920]	train's l1: 2.0327	test's l1: 3.5311
[930]	train's l1: 2.0311	test's l1: 3.53108
[940]	train's l1: 2.03062	test's l1: 3.53095
[950]	train's l1: 2.03009	test's l1: 3.53079
[960]	train's l1: 2.01509	test's l1: 3.50936
[970]	train's l1: 2.01396	test's l1: 3.50918
[980]	train's l1: 2.00621	test's l1: 3.50803
[990]	train's l1: 2.00516	test's l1: 3.5073
[1000]	train's l1: 2.00446	test's l1: 3.50679
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.00446	test's l1: 3.50679
Starting for w140_False with mul=7
140: 52m40sec done
140: 52m50sec done
140: 53m0sec done
140: 53m10sec done
140: 53m20sec done
140: 53m30sec done
140: 53m40sec done
140: 53m50sec done
140: 54m0sec done
140: 54m10sec done
140: 54m20sec done
140: 54m30sec done
140: 54m40sec done
140: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.221403 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1604400, number of used features: 247
[LightGBM] [Info] Start training from score 71.897499
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4264	test's l1: 61.3736
[20]	train's l1: 39.9527	test's l1: 39.9636
[30]	train's l1: 26.4846	test's l1: 26.5167
[40]	train's l1: 18.9152	test's l1: 19.1662
[50]	train's l1: 11.2055	test's l1: 11.3648
[60]	train's l1: 7.52701	test's l1: 7.82558
[70]	train's l1: 6.65611	test's l1: 7.18214
[80]	train's l1: 6.06141	test's l1: 6.6904
[90]	train's l1: 5.83751	test's l1: 6.47751
[100]	train's l1: 4.83406	test's l1: 5.57062
[110]	train's l1: 4.41087	test's l1: 5.0889
[120]	train's l1: 4.21656	test's l1: 4.8526
[130]	train's l1: 4.01486	test's l1: 4.64719
[140]	train's l1: 3.96899	test's l1: 4.60886
[150]	train's l1: 3.9341	test's l1: 4.57548
[160]	train's l1: 3.92352	test's l1: 4.56639
[170]	train's l1: 3.87559	test's l1: 4.5255
[180]	train's l1: 3.84524	test's l1: 4.50355
[190]	train's l1: 3.77654	test's l1: 4.45928
[200]	train's l1: 3.67388	test's l1: 4.38025
[210]	train's l1: 3.56785	test's l1: 4.30145
[220]	train's l1: 3.51377	test's l1: 4.28442
[230]	train's l1: 3.50983	test's l1: 4.27847
[240]	train's l1: 3.40369	test's l1: 4.20792
[250]	train's l1: 3.36099	test's l1: 4.19664
[260]	train's l1: 3.35503	test's l1: 4.18266
[270]	train's l1: 3.3446	test's l1: 4.19042
[280]	train's l1: 3.34345	test's l1: 4.1905
[290]	train's l1: 3.32475	test's l1: 4.18079
[300]	train's l1: 3.31151	test's l1: 4.17894
[310]	train's l1: 3.30118	test's l1: 4.16798
[320]	train's l1: 3.25777	test's l1: 4.14221
[330]	train's l1: 3.24857	test's l1: 4.1313
[340]	train's l1: 3.23923	test's l1: 4.13213
[350]	train's l1: 3.20888	test's l1: 4.12947
[360]	train's l1: 3.1948	test's l1: 4.11988
[370]	train's l1: 3.18279	test's l1: 4.11628
[380]	train's l1: 3.16703	test's l1: 4.11454
[390]	train's l1: 3.15251	test's l1: 4.10121
[400]	train's l1: 3.13233	test's l1: 4.09133
[410]	train's l1: 3.1117	test's l1: 4.08857
[420]	train's l1: 3.10086	test's l1: 4.08173
[430]	train's l1: 2.98871	test's l1: 4.00899
[440]	train's l1: 2.98509	test's l1: 4.00862
[450]	train's l1: 2.97122	test's l1: 4.00337
[460]	train's l1: 2.96609	test's l1: 4.00011
[470]	train's l1: 2.94945	test's l1: 3.99396
[480]	train's l1: 2.94066	test's l1: 3.99268
[490]	train's l1: 2.93702	test's l1: 3.99152
[500]	train's l1: 2.93619	test's l1: 3.99158
[510]	train's l1: 2.77064	test's l1: 3.89015
[520]	train's l1: 2.70695	test's l1: 3.85396
[530]	train's l1: 2.69802	test's l1: 3.84286
[540]	train's l1: 2.59107	test's l1: 3.73443
[550]	train's l1: 2.50371	test's l1: 3.68749
[560]	train's l1: 2.48464	test's l1: 3.66672
[570]	train's l1: 2.45997	test's l1: 3.65151
[580]	train's l1: 2.45794	test's l1: 3.65017
[590]	train's l1: 2.4565	test's l1: 3.64924
[600]	train's l1: 2.45377	test's l1: 3.64803
[610]	train's l1: 2.4436	test's l1: 3.63292
[620]	train's l1: 2.44261	test's l1: 3.63304
[630]	train's l1: 2.44121	test's l1: 3.63267
[640]	train's l1: 2.43987	test's l1: 3.63189
[650]	train's l1: 2.4394	test's l1: 3.63134
[660]	train's l1: 2.42936	test's l1: 3.62412
[670]	train's l1: 2.42773	test's l1: 3.62307
[680]	train's l1: 2.42633	test's l1: 3.62237
[690]	train's l1: 2.41684	test's l1: 3.61594
[700]	train's l1: 2.39716	test's l1: 3.60162
[710]	train's l1: 2.38045	test's l1: 3.58929
[720]	train's l1: 2.3766	test's l1: 3.58647
[730]	train's l1: 2.37449	test's l1: 3.58674
[740]	train's l1: 2.37016	test's l1: 3.58361
[750]	train's l1: 2.33705	test's l1: 3.56046
[760]	train's l1: 2.31661	test's l1: 3.55195
[770]	train's l1: 2.31572	test's l1: 3.55134
[780]	train's l1: 2.3066	test's l1: 3.55076
[790]	train's l1: 2.30523	test's l1: 3.54991
[800]	train's l1: 2.29742	test's l1: 3.5344
[810]	train's l1: 2.29487	test's l1: 3.53399
[820]	train's l1: 2.29246	test's l1: 3.53387
[830]	train's l1: 2.28946	test's l1: 3.53204
[840]	train's l1: 2.28282	test's l1: 3.52738
[850]	train's l1: 2.28132	test's l1: 3.52731
[860]	train's l1: 2.28034	test's l1: 3.52656
[870]	train's l1: 2.27678	test's l1: 3.52787
[880]	train's l1: 2.27093	test's l1: 3.52356
[890]	train's l1: 2.26951	test's l1: 3.52329
[900]	train's l1: 2.26734	test's l1: 3.5224
[910]	train's l1: 2.2667	test's l1: 3.52235
[920]	train's l1: 2.26554	test's l1: 3.52212
[930]	train's l1: 2.26197	test's l1: 3.52403
[940]	train's l1: 2.261	test's l1: 3.52363
[950]	train's l1: 2.26037	test's l1: 3.52354
[960]	train's l1: 2.24593	test's l1: 3.51562
[970]	train's l1: 2.24504	test's l1: 3.51554
[980]	train's l1: 2.2445	test's l1: 3.51551
[990]	train's l1: 2.24232	test's l1: 3.51497
[1000]	train's l1: 2.24024	test's l1: 3.51385
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.24024	test's l1: 3.51385
Starting for w120_False with mul=7
120: 53m0sec done
120: 53m10sec done
120: 53m20sec done
120: 53m30sec done
120: 53m40sec done
120: 53m50sec done
120: 54m0sec done
120: 54m10sec done
120: 54m20sec done
120: 54m30sec done
120: 54m40sec done
120: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.223332 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 1772400, number of used features: 247
[LightGBM] [Info] Start training from score 71.892502
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.3921	test's l1: 61.3266
[20]	train's l1: 39.9159	test's l1: 39.9241
[30]	train's l1: 26.5025	test's l1: 26.5566
[40]	train's l1: 18.3064	test's l1: 18.6236
[50]	train's l1: 11.1641	test's l1: 11.416
[60]	train's l1: 7.84073	test's l1: 8.14158
[70]	train's l1: 6.66723	test's l1: 7.25239
[80]	train's l1: 5.3677	test's l1: 6.18534
[90]	train's l1: 4.73175	test's l1: 5.59194
[100]	train's l1: 4.25336	test's l1: 5.14826
[110]	train's l1: 3.97434	test's l1: 4.92398
[120]	train's l1: 3.96158	test's l1: 4.91795
[130]	train's l1: 3.89794	test's l1: 4.85461
[140]	train's l1: 3.87298	test's l1: 4.836
[150]	train's l1: 3.62884	test's l1: 4.64675
[160]	train's l1: 3.52816	test's l1: 4.56738
[170]	train's l1: 3.51926	test's l1: 4.55617
[180]	train's l1: 3.50143	test's l1: 4.54139
[190]	train's l1: 3.47468	test's l1: 4.53336
[200]	train's l1: 3.40655	test's l1: 4.49278
[210]	train's l1: 3.36887	test's l1: 4.4793
[220]	train's l1: 3.33261	test's l1: 4.40777
[230]	train's l1: 3.31683	test's l1: 4.39874
[240]	train's l1: 3.30456	test's l1: 4.38894
[250]	train's l1: 3.3029	test's l1: 4.388
[260]	train's l1: 3.22492	test's l1: 4.347
[270]	train's l1: 3.21143	test's l1: 4.35414
[280]	train's l1: 3.20144	test's l1: 4.35046
[290]	train's l1: 3.19126	test's l1: 4.35055
[300]	train's l1: 3.13771	test's l1: 4.32561
[310]	train's l1: 2.91095	test's l1: 4.16403
[320]	train's l1: 2.90425	test's l1: 4.16175
[330]	train's l1: 2.89866	test's l1: 4.15147
[340]	train's l1: 2.89094	test's l1: 4.1493
[350]	train's l1: 2.87836	test's l1: 4.1263
[360]	train's l1: 2.82169	test's l1: 4.07622
[370]	train's l1: 2.8189	test's l1: 4.07541
[380]	train's l1: 2.76338	test's l1: 4.03531
[390]	train's l1: 2.76075	test's l1: 4.03461
[400]	train's l1: 2.75927	test's l1: 4.0342
[410]	train's l1: 2.71334	test's l1: 3.99265
[420]	train's l1: 2.71232	test's l1: 3.99186
[430]	train's l1: 2.71007	test's l1: 3.99206
[440]	train's l1: 2.70463	test's l1: 3.9928
[450]	train's l1: 2.69773	test's l1: 3.98916
[460]	train's l1: 2.69397	test's l1: 3.98645
[470]	train's l1: 2.69054	test's l1: 3.98433
[480]	train's l1: 2.68723	test's l1: 3.98601
[490]	train's l1: 2.68324	test's l1: 3.98401
[500]	train's l1: 2.68243	test's l1: 3.98395
[510]	train's l1: 2.67865	test's l1: 3.98339
[520]	train's l1: 2.67352	test's l1: 3.98442
[530]	train's l1: 2.67249	test's l1: 3.98419
[540]	train's l1: 2.66912	test's l1: 3.98367
[550]	train's l1: 2.66634	test's l1: 3.98719
[560]	train's l1: 2.65448	test's l1: 3.97912
[570]	train's l1: 2.64367	test's l1: 3.96844
[580]	train's l1: 2.63612	test's l1: 3.96747
[590]	train's l1: 2.63088	test's l1: 3.95878
[600]	train's l1: 2.62498	test's l1: 3.9589
[610]	train's l1: 2.62073	test's l1: 3.95755
[620]	train's l1: 2.6011	test's l1: 3.94532
[630]	train's l1: 2.55561	test's l1: 3.90325
[640]	train's l1: 2.52379	test's l1: 3.88289
[650]	train's l1: 2.5189	test's l1: 3.88017
[660]	train's l1: 2.51732	test's l1: 3.87957
[670]	train's l1: 2.50872	test's l1: 3.8745
[680]	train's l1: 2.5026	test's l1: 3.87013
[690]	train's l1: 2.49132	test's l1: 3.86709
[700]	train's l1: 2.48495	test's l1: 3.86194
[710]	train's l1: 2.44269	test's l1: 3.81342
[720]	train's l1: 2.44158	test's l1: 3.81304
[730]	train's l1: 2.43813	test's l1: 3.8137
[740]	train's l1: 2.412	test's l1: 3.78489
[750]	train's l1: 2.40694	test's l1: 3.78505
[760]	train's l1: 2.38405	test's l1: 3.77713
[770]	train's l1: 2.37107	test's l1: 3.76892
[780]	train's l1: 2.36992	test's l1: 3.76822
[790]	train's l1: 2.36502	test's l1: 3.76422
[800]	train's l1: 2.36254	test's l1: 3.76262
[810]	train's l1: 2.36004	test's l1: 3.76551
[820]	train's l1: 2.33344	test's l1: 3.75237
[830]	train's l1: 2.32614	test's l1: 3.74806
[840]	train's l1: 2.32561	test's l1: 3.74803
[850]	train's l1: 2.30584	test's l1: 3.7239
[860]	train's l1: 2.25275	test's l1: 3.69017
[870]	train's l1: 2.25052	test's l1: 3.68962
[880]	train's l1: 2.23562	test's l1: 3.67744
[890]	train's l1: 2.23516	test's l1: 3.67733
[900]	train's l1: 2.23465	test's l1: 3.67744
[910]	train's l1: 2.23259	test's l1: 3.67792
[920]	train's l1: 2.23169	test's l1: 3.67759
[930]	train's l1: 2.23009	test's l1: 3.67592
[940]	train's l1: 2.22954	test's l1: 3.67599
[950]	train's l1: 2.22725	test's l1: 3.67449
[960]	train's l1: 2.22183	test's l1: 3.66635
[970]	train's l1: 2.22101	test's l1: 3.66607
[980]	train's l1: 2.22019	test's l1: 3.66532
[990]	train's l1: 2.2186	test's l1: 3.66488
[1000]	train's l1: 2.21705	test's l1: 3.66484
Did not meet early stopping. Best iteration is:
[996]	train's l1: 2.2183	test's l1: 3.6648
Starting for w100_False with mul=7
Starting for w80_False with mul=7
80: 53m40sec done
80: 53m50sec done
80: 54m0sec done
80: 54m10sec done
80: 54m20sec done
80: 54m30sec done
80: 54m40sec done
80: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251898 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 2108400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.3775	test's l1: 61.32
[20]	train's l1: 39.7973	test's l1: 39.858
[30]	train's l1: 26.4361	test's l1: 26.5369
[40]	train's l1: 18.2827	test's l1: 18.6228
[50]	train's l1: 11.1148	test's l1: 11.3981
[60]	train's l1: 7.8094	test's l1: 8.18435
[70]	train's l1: 6.55081	test's l1: 7.01833
[80]	train's l1: 5.63904	test's l1: 6.21143
[90]	train's l1: 4.77345	test's l1: 5.4536
[100]	train's l1: 4.25315	test's l1: 4.92679
[110]	train's l1: 4.18002	test's l1: 4.87234
[120]	train's l1: 4.02405	test's l1: 4.71593
[130]	train's l1: 4.01823	test's l1: 4.71086
[140]	train's l1: 3.86357	test's l1: 4.58507
[150]	train's l1: 3.85558	test's l1: 4.57907
[160]	train's l1: 3.69439	test's l1: 4.43656
[170]	train's l1: 3.51809	test's l1: 4.27461
[180]	train's l1: 3.3638	test's l1: 4.19386
[190]	train's l1: 3.34512	test's l1: 4.18657
[200]	train's l1: 3.31918	test's l1: 4.16385
[210]	train's l1: 3.31817	test's l1: 4.1641
[220]	train's l1: 3.31422	test's l1: 4.15932
[230]	train's l1: 3.30348	test's l1: 4.15626
[240]	train's l1: 3.24966	test's l1: 4.13779
[250]	train's l1: 3.2433	test's l1: 4.13384
[260]	train's l1: 3.10446	test's l1: 4.04036
[270]	train's l1: 2.94679	test's l1: 3.91333
[280]	train's l1: 2.90927	test's l1: 3.88779
[290]	train's l1: 2.90775	test's l1: 3.88639
[300]	train's l1: 2.90489	test's l1: 3.88352
[310]	train's l1: 2.90389	test's l1: 3.88263
[320]	train's l1: 2.90046	test's l1: 3.87825
[330]	train's l1: 2.89756	test's l1: 3.87791
[340]	train's l1: 2.89559	test's l1: 3.87659
[350]	train's l1: 2.87908	test's l1: 3.86705
[360]	train's l1: 2.8272	test's l1: 3.83448
[370]	train's l1: 2.81962	test's l1: 3.82841
[380]	train's l1: 2.79712	test's l1: 3.81597
[390]	train's l1: 2.79472	test's l1: 3.81618
[400]	train's l1: 2.78386	test's l1: 3.81697
[410]	train's l1: 2.712	test's l1: 3.77974
[420]	train's l1: 2.7077	test's l1: 3.77613
[430]	train's l1: 2.69139	test's l1: 3.77246
[440]	train's l1: 2.68996	test's l1: 3.77201
[450]	train's l1: 2.68252	test's l1: 3.76328
[460]	train's l1: 2.63129	test's l1: 3.75226
[470]	train's l1: 2.6085	test's l1: 3.74737
[480]	train's l1: 2.60582	test's l1: 3.74714
[490]	train's l1: 2.54748	test's l1: 3.70234
[500]	train's l1: 2.52401	test's l1: 3.69022
[510]	train's l1: 2.51649	test's l1: 3.68664
[520]	train's l1: 2.51094	test's l1: 3.68942
[530]	train's l1: 2.48144	test's l1: 3.66153
[540]	train's l1: 2.46882	test's l1: 3.65556
[550]	train's l1: 2.45639	test's l1: 3.65472
[560]	train's l1: 2.45075	test's l1: 3.65207
[570]	train's l1: 2.42929	test's l1: 3.64014
[580]	train's l1: 2.37479	test's l1: 3.59308
[590]	train's l1: 2.37409	test's l1: 3.59265
[600]	train's l1: 2.37076	test's l1: 3.59221
[610]	train's l1: 2.35702	test's l1: 3.58848
[620]	train's l1: 2.35211	test's l1: 3.58742
[630]	train's l1: 2.34959	test's l1: 3.58482
[640]	train's l1: 2.34503	test's l1: 3.58548
[650]	train's l1: 2.34079	test's l1: 3.58254
[660]	train's l1: 2.33141	test's l1: 3.57827
[670]	train's l1: 2.2872	test's l1: 3.53108
[680]	train's l1: 2.28385	test's l1: 3.52764
[690]	train's l1: 2.28248	test's l1: 3.52783
[700]	train's l1: 2.27807	test's l1: 3.528
[710]	train's l1: 2.27268	test's l1: 3.52784
[720]	train's l1: 2.26825	test's l1: 3.52623
[730]	train's l1: 2.24537	test's l1: 3.51313
[740]	train's l1: 2.24255	test's l1: 3.5023
[750]	train's l1: 2.21964	test's l1: 3.4847
[760]	train's l1: 2.20032	test's l1: 3.46573
[770]	train's l1: 2.1979	test's l1: 3.46697
[780]	train's l1: 2.19622	test's l1: 3.46724
[790]	train's l1: 2.19293	test's l1: 3.46661
[800]	train's l1: 2.19172	test's l1: 3.46656
[810]	train's l1: 2.18836	test's l1: 3.46714
[820]	train's l1: 2.17682	test's l1: 3.45702
[830]	train's l1: 2.1337	test's l1: 3.42934
[840]	train's l1: 2.13031	test's l1: 3.42777
[850]	train's l1: 2.12884	test's l1: 3.427
[860]	train's l1: 2.12821	test's l1: 3.42659
[870]	train's l1: 2.12667	test's l1: 3.42574
[880]	train's l1: 2.12312	test's l1: 3.42414
[890]	train's l1: 2.1225	test's l1: 3.42365
[900]	train's l1: 2.12208	test's l1: 3.42329
[910]	train's l1: 2.12103	test's l1: 3.42274
[920]	train's l1: 2.11656	test's l1: 3.42008
[930]	train's l1: 2.11569	test's l1: 3.41973
[940]	train's l1: 2.11472	test's l1: 3.419
[950]	train's l1: 2.11409	test's l1: 3.41897
[960]	train's l1: 2.11348	test's l1: 3.41905
[970]	train's l1: 2.08196	test's l1: 3.405
[980]	train's l1: 2.02468	test's l1: 3.3619
[990]	train's l1: 2.01939	test's l1: 3.36014
[1000]	train's l1: 2.0134	test's l1: 3.35522
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 2.0134	test's l1: 3.35522
Starting for w60_False with mul=7
60: 54m0sec done
60: 54m10sec done
60: 54m20sec done
60: 54m30sec done
60: 54m40sec done
60: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.251434 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 2276400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4125	test's l1: 61.3824
[20]	train's l1: 39.9081	test's l1: 39.9482
[30]	train's l1: 26.4701	test's l1: 26.5648
[40]	train's l1: 18.2538	test's l1: 18.6127
[50]	train's l1: 11.2029	test's l1: 11.3625
[60]	train's l1: 7.88385	test's l1: 8.0635
[70]	train's l1: 6.05587	test's l1: 6.35523
[80]	train's l1: 4.80399	test's l1: 5.25433
[90]	train's l1: 4.04288	test's l1: 4.58868
[100]	train's l1: 3.9935	test's l1: 4.51831
[110]	train's l1: 3.97135	test's l1: 4.49111
[120]	train's l1: 3.93875	test's l1: 4.46705
[130]	train's l1: 3.93513	test's l1: 4.46438
[140]	train's l1: 3.77302	test's l1: 4.24416
[150]	train's l1: 3.53211	test's l1: 4.12954
[160]	train's l1: 3.39304	test's l1: 4.05825
[170]	train's l1: 3.38521	test's l1: 4.05447
[180]	train's l1: 3.38308	test's l1: 4.05363
[190]	train's l1: 3.1964	test's l1: 3.92228
[200]	train's l1: 3.18803	test's l1: 3.90988
[210]	train's l1: 3.15389	test's l1: 3.86114
[220]	train's l1: 3.15165	test's l1: 3.86104
[230]	train's l1: 3.10423	test's l1: 3.80458
[240]	train's l1: 3.07561	test's l1: 3.76327
[250]	train's l1: 3.05017	test's l1: 3.72801
[260]	train's l1: 3.02181	test's l1: 3.69156
[270]	train's l1: 2.99785	test's l1: 3.65528
[280]	train's l1: 2.99538	test's l1: 3.6527
[290]	train's l1: 2.99238	test's l1: 3.65081
[300]	train's l1: 2.99075	test's l1: 3.64976
[310]	train's l1: 2.98475	test's l1: 3.64883
[320]	train's l1: 2.97573	test's l1: 3.64323
[330]	train's l1: 2.88667	test's l1: 3.52705
[340]	train's l1: 2.87179	test's l1: 3.51684
[350]	train's l1: 2.8677	test's l1: 3.52153
[360]	train's l1: 2.8629	test's l1: 3.51999
[370]	train's l1: 2.85776	test's l1: 3.51656
[380]	train's l1: 2.82364	test's l1: 3.49258
[390]	train's l1: 2.76172	test's l1: 3.43862
[400]	train's l1: 2.71123	test's l1: 3.41473
[410]	train's l1: 2.69728	test's l1: 3.40303
[420]	train's l1: 2.60927	test's l1: 3.31392
[430]	train's l1: 2.6076	test's l1: 3.31391
[440]	train's l1: 2.57903	test's l1: 3.29683
[450]	train's l1: 2.5244	test's l1: 3.26311
[460]	train's l1: 2.52344	test's l1: 3.26324
[470]	train's l1: 2.48001	test's l1: 3.22577
[480]	train's l1: 2.35026	test's l1: 3.17008
[490]	train's l1: 2.32357	test's l1: 3.15557
[500]	train's l1: 2.32212	test's l1: 3.15536
[510]	train's l1: 2.31315	test's l1: 3.15081
[520]	train's l1: 2.30418	test's l1: 3.14432
[530]	train's l1: 2.30159	test's l1: 3.14418
[540]	train's l1: 2.30058	test's l1: 3.14441
[550]	train's l1: 2.29622	test's l1: 3.14172
[560]	train's l1: 2.29365	test's l1: 3.14078
[570]	train's l1: 2.2928	test's l1: 3.14061
[580]	train's l1: 2.29117	test's l1: 3.14035
[590]	train's l1: 2.28106	test's l1: 3.13718
[600]	train's l1: 2.28012	test's l1: 3.13703
[610]	train's l1: 2.2766	test's l1: 3.13413
[620]	train's l1: 2.27255	test's l1: 3.13203
[630]	train's l1: 2.27104	test's l1: 3.13421
[640]	train's l1: 2.27006	test's l1: 3.1339
[650]	train's l1: 2.26833	test's l1: 3.13397
[660]	train's l1: 2.26478	test's l1: 3.13436
[670]	train's l1: 2.25024	test's l1: 3.12462
[680]	train's l1: 2.24271	test's l1: 3.11578
[690]	train's l1: 2.22769	test's l1: 3.10915
[700]	train's l1: 2.22433	test's l1: 3.10737
[710]	train's l1: 2.22361	test's l1: 3.10703
[720]	train's l1: 2.22196	test's l1: 3.10899
[730]	train's l1: 2.21783	test's l1: 3.11146
[740]	train's l1: 2.21736	test's l1: 3.11149
[750]	train's l1: 2.21689	test's l1: 3.11135
[760]	train's l1: 2.21037	test's l1: 3.10567
[770]	train's l1: 2.20433	test's l1: 3.10343
[780]	train's l1: 2.20322	test's l1: 3.1033
[790]	train's l1: 2.20143	test's l1: 3.10201
[800]	train's l1: 2.2008	test's l1: 3.10193
[810]	train's l1: 2.19774	test's l1: 3.10137
[820]	train's l1: 2.19512	test's l1: 3.10057
[830]	train's l1: 2.19407	test's l1: 3.10042
[840]	train's l1: 2.18963	test's l1: 3.09916
[850]	train's l1: 2.18878	test's l1: 3.09873
[860]	train's l1: 2.18758	test's l1: 3.09815
[870]	train's l1: 2.18546	test's l1: 3.09698
[880]	train's l1: 2.1846	test's l1: 3.09673
[890]	train's l1: 2.18266	test's l1: 3.09622
[900]	train's l1: 2.18125	test's l1: 3.09603
[910]	train's l1: 2.17913	test's l1: 3.09625
[920]	train's l1: 2.1507	test's l1: 3.08503
[930]	train's l1: 2.15	test's l1: 3.08484
[940]	train's l1: 2.13965	test's l1: 3.07873
[950]	train's l1: 2.13178	test's l1: 3.07978
[960]	train's l1: 2.1264	test's l1: 3.07565
[970]	train's l1: 2.12452	test's l1: 3.0746
[980]	train's l1: 2.086	test's l1: 3.06053
[990]	train's l1: 1.97294	test's l1: 3.03372
[1000]	train's l1: 1.9523	test's l1: 3.02157
Did not meet early stopping. Best iteration is:
[992]	train's l1: 1.95547	test's l1: 3.02082
Starting for w40_False with mul=7
40: 54m20sec done
40: 54m30sec done
40: 54m40sec done
40: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.282293 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 2444400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4737	test's l1: 61.4227
[20]	train's l1: 39.91	test's l1: 39.9269
[30]	train's l1: 26.5025	test's l1: 26.5781
[40]	train's l1: 18.8486	test's l1: 19.1524
[50]	train's l1: 11.1908	test's l1: 11.3409
[60]	train's l1: 7.07314	test's l1: 7.5735
[70]	train's l1: 6.75664	test's l1: 7.25566
[80]	train's l1: 5.86363	test's l1: 6.36146
[90]	train's l1: 4.53912	test's l1: 5.30717
[100]	train's l1: 3.36176	test's l1: 4.15854
[110]	train's l1: 3.22815	test's l1: 4.0295
[120]	train's l1: 3.21596	test's l1: 4.01798
[130]	train's l1: 3.19627	test's l1: 3.99723
[140]	train's l1: 3.17455	test's l1: 3.98693
[150]	train's l1: 3.16369	test's l1: 3.98244
[160]	train's l1: 3.14033	test's l1: 3.95533
[170]	train's l1: 2.9399	test's l1: 3.79906
[180]	train's l1: 2.91805	test's l1: 3.78097
[190]	train's l1: 2.89249	test's l1: 3.73824
[200]	train's l1: 2.74582	test's l1: 3.60171
[210]	train's l1: 2.72389	test's l1: 3.57895
[220]	train's l1: 2.70784	test's l1: 3.54976
[230]	train's l1: 2.69878	test's l1: 3.5407
[240]	train's l1: 2.68108	test's l1: 3.51416
[250]	train's l1: 2.64645	test's l1: 3.49575
[260]	train's l1: 2.63749	test's l1: 3.49151
[270]	train's l1: 2.6361	test's l1: 3.49147
[280]	train's l1: 2.62625	test's l1: 3.48205
[290]	train's l1: 2.62448	test's l1: 3.48158
[300]	train's l1: 2.56723	test's l1: 3.43663
[310]	train's l1: 2.55947	test's l1: 3.42907
[320]	train's l1: 2.55098	test's l1: 3.42155
[330]	train's l1: 2.54733	test's l1: 3.42091
[340]	train's l1: 2.53849	test's l1: 3.41691
[350]	train's l1: 2.53373	test's l1: 3.41316
[360]	train's l1: 2.52513	test's l1: 3.40936
[370]	train's l1: 2.50647	test's l1: 3.39256
[380]	train's l1: 2.45186	test's l1: 3.36556
[390]	train's l1: 2.45086	test's l1: 3.36536
[400]	train's l1: 2.45005	test's l1: 3.36492
[410]	train's l1: 2.44479	test's l1: 3.36252
[420]	train's l1: 2.43749	test's l1: 3.35599
[430]	train's l1: 2.42763	test's l1: 3.35613
[440]	train's l1: 2.42674	test's l1: 3.35571
[450]	train's l1: 2.42535	test's l1: 3.35447
[460]	train's l1: 2.41903	test's l1: 3.34769
[470]	train's l1: 2.38533	test's l1: 3.28839
[480]	train's l1: 2.3802	test's l1: 3.28517
[490]	train's l1: 2.37126	test's l1: 3.27862
[500]	train's l1: 2.36459	test's l1: 3.27372
[510]	train's l1: 2.36027	test's l1: 3.27347
[520]	train's l1: 2.28172	test's l1: 3.21251
[530]	train's l1: 2.2794	test's l1: 3.21276
[540]	train's l1: 2.27447	test's l1: 3.20913
[550]	train's l1: 2.27133	test's l1: 3.20698
[560]	train's l1: 2.26298	test's l1: 3.2048
[570]	train's l1: 2.24728	test's l1: 3.19007
[580]	train's l1: 2.21378	test's l1: 3.16605
[590]	train's l1: 2.20942	test's l1: 3.1663
[600]	train's l1: 2.19692	test's l1: 3.15508
[610]	train's l1: 2.1831	test's l1: 3.14835
[620]	train's l1: 2.17941	test's l1: 3.14385
[630]	train's l1: 2.17537	test's l1: 3.14217
[640]	train's l1: 2.15849	test's l1: 3.13817
[650]	train's l1: 2.07518	test's l1: 3.10482
[660]	train's l1: 2.05681	test's l1: 3.09731
[670]	train's l1: 2.01919	test's l1: 3.07458
[680]	train's l1: 2.01474	test's l1: 3.07144
[690]	train's l1: 2.01412	test's l1: 3.07089
[700]	train's l1: 2.01364	test's l1: 3.0708
[710]	train's l1: 2.01182	test's l1: 3.07069
[720]	train's l1: 2.00374	test's l1: 3.06608
[730]	train's l1: 2.00004	test's l1: 3.06501
[740]	train's l1: 1.99981	test's l1: 3.06488
[750]	train's l1: 1.99268	test's l1: 3.06154
[760]	train's l1: 1.99057	test's l1: 3.06043
[770]	train's l1: 1.98871	test's l1: 3.05973
[780]	train's l1: 1.98809	test's l1: 3.05962
[790]	train's l1: 1.9742	test's l1: 3.04894
[800]	train's l1: 1.97374	test's l1: 3.04895
[810]	train's l1: 1.97206	test's l1: 3.04893
[820]	train's l1: 1.97099	test's l1: 3.04904
[830]	train's l1: 1.96825	test's l1: 3.04726
[840]	train's l1: 1.9613	test's l1: 3.04364
[850]	train's l1: 1.95097	test's l1: 3.0371
[860]	train's l1: 1.94456	test's l1: 3.03148
[870]	train's l1: 1.94208	test's l1: 3.02983
[880]	train's l1: 1.94076	test's l1: 3.02906
[890]	train's l1: 1.9372	test's l1: 3.02857
[900]	train's l1: 1.93663	test's l1: 3.02906
[910]	train's l1: 1.93588	test's l1: 3.02865
[920]	train's l1: 1.9357	test's l1: 3.02854
[930]	train's l1: 1.92991	test's l1: 3.0277
[940]	train's l1: 1.92829	test's l1: 3.02878
[950]	train's l1: 1.92677	test's l1: 3.029
[960]	train's l1: 1.92603	test's l1: 3.02866
[970]	train's l1: 1.92191	test's l1: 3.02889
[980]	train's l1: 1.92073	test's l1: 3.02807
[990]	train's l1: 1.9206	test's l1: 3.02808
[1000]	train's l1: 1.91652	test's l1: 3.02753
Did not meet early stopping. Best iteration is:
[1000]	train's l1: 1.91652	test's l1: 3.02753
Starting for w20_False with mul=7
20: 54m40sec done
20: 54m50sec done
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.294253 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 60530
[LightGBM] [Info] Number of data points in the train set: 2612400, number of used features: 247
[LightGBM] [Info] Start training from score 71.889999
Training until validation scores don't improve for 100 rounds
[10]	train's l1: 61.4549	test's l1: 61.3999
[20]	train's l1: 39.8788	test's l1: 39.8997
[30]	train's l1: 26.4862	test's l1: 26.5551
[40]	train's l1: 18.215	test's l1: 18.5314
[50]	train's l1: 11.0935	test's l1: 11.3209
[60]	train's l1: 7.83547	test's l1: 8.15175
[70]	train's l1: 5.53037	test's l1: 6.43695
[80]	train's l1: 4.46554	test's l1: 5.69428
[90]	train's l1: 4.04907	test's l1: 5.33822
[100]	train's l1: 3.98294	test's l1: 5.2598
[110]	train's l1: 3.75188	test's l1: 5.02832
[120]	train's l1: 3.69014	test's l1: 4.98806
[130]	train's l1: 3.60355	test's l1: 4.93001
[140]	train's l1: 3.56616	test's l1: 4.89511
[150]	train's l1: 3.48626	test's l1: 4.79124
[160]	train's l1: 3.40474	test's l1: 4.72996
[170]	train's l1: 3.38177	test's l1: 4.71165
[180]	train's l1: 3.36731	test's l1: 4.70503
[190]	train's l1: 3.11856	test's l1: 4.37233
[200]	train's l1: 3.10828	test's l1: 4.36123
[210]	train's l1: 3.08211	test's l1: 4.35346
[220]	train's l1: 3.07403	test's l1: 4.34549
[230]	train's l1: 3.06501	test's l1: 4.34001
[240]	train's l1: 3.02502	test's l1: 4.31997
[250]	train's l1: 3.01809	test's l1: 4.31837
[260]	train's l1: 2.99595	test's l1: 4.31096
[270]	train's l1: 2.99572	test's l1: 4.31112
[280]	train's l1: 2.99485	test's l1: 4.31094
[290]	train's l1: 2.98383	test's l1: 4.30336
[300]	train's l1: 2.98025	test's l1: 4.30102
[310]	train's l1: 2.97682	test's l1: 4.30152
[320]	train's l1: 2.90376	test's l1: 4.20817
[330]	train's l1: 2.9007	test's l1: 4.20665
[340]	train's l1: 2.89281	test's l1: 4.19778
[350]	train's l1: 2.78172	test's l1: 4.13514
[360]	train's l1: 2.77477	test's l1: 4.13231
[370]	train's l1: 2.77328	test's l1: 4.13142
[380]	train's l1: 2.76968	test's l1: 4.12913
[390]	train's l1: 2.76497	test's l1: 4.12781
[400]	train's l1: 2.74649	test's l1: 4.11812
[410]	train's l1: 2.74471	test's l1: 4.11355
[420]	train's l1: 2.72285	test's l1: 4.09933
[430]	train's l1: 2.72026	test's l1: 4.09866
[440]	train's l1: 2.65912	test's l1: 4.01796
[450]	train's l1: 2.64179	test's l1: 4.00286
[460]	train's l1: 2.63942	test's l1: 4.00241
[470]	train's l1: 2.60451	test's l1: 3.97905
[480]	train's l1: 2.60416	test's l1: 3.97891
[490]	train's l1: 2.55421	test's l1: 3.93415
[500]	train's l1: 2.54525	test's l1: 3.93175
[510]	train's l1: 2.52877	test's l1: 3.93253
[520]	train's l1: 2.49895	test's l1: 3.8849
[530]	train's l1: 2.49125	test's l1: 3.88472
[540]	train's l1: 2.49027	test's l1: 3.88435
[550]	train's l1: 2.48454	test's l1: 3.87935
[560]	train's l1: 2.4817	test's l1: 3.87768
[570]	train's l1: 2.43469	test's l1: 3.84422
[580]	train's l1: 2.42724	test's l1: 3.8374
[590]	train's l1: 2.42553	test's l1: 3.83717
[600]	train's l1: 2.41616	test's l1: 3.83184
[610]	train's l1: 2.41267	test's l1: 3.82919
[620]	train's l1: 2.4106	test's l1: 3.8294
[630]	train's l1: 2.40385	test's l1: 3.82627
[640]	train's l1: 2.39879	test's l1: 3.82478
[650]	train's l1: 2.39481	test's l1: 3.82361
[660]	train's l1: 2.39147	test's l1: 3.82288
[670]	train's l1: 2.38921	test's l1: 3.822
[680]	train's l1: 2.34275	test's l1: 3.75581
[690]	train's l1: 2.3224	test's l1: 3.73664
[700]	train's l1: 2.1699	test's l1: 3.67488
[710]	train's l1: 2.16923	test's l1: 3.67473
[720]	train's l1: 2.12285	test's l1: 3.64902
[730]	train's l1: 2.11749	test's l1: 3.64395
[740]	train's l1: 2.11334	test's l1: 3.64226
[750]	train's l1: 2.10324	test's l1: 3.64721
[760]	train's l1: 2.10271	test's l1: 3.6467
[770]	train's l1: 2.10218	test's l1: 3.64684
[780]	train's l1: 2.10166	test's l1: 3.64686
[790]	train's l1: 2.10019	test's l1: 3.64576
[800]	train's l1: 2.07839	test's l1: 3.63211
[810]	train's l1: 2.07716	test's l1: 3.63116
[820]	train's l1: 2.07393	test's l1: 3.63132
[830]	train's l1: 2.07243	test's l1: 3.63218
[840]	train's l1: 2.06255	test's l1: 3.63629
[850]	train's l1: 2.05401	test's l1: 3.62552
[860]	train's l1: 2.04943	test's l1: 3.62545
[870]	train's l1: 2.03842	test's l1: 3.62229
[880]	train's l1: 2.03774	test's l1: 3.62218
[890]	train's l1: 2.02429	test's l1: 3.61451
[900]	train's l1: 2.02333	test's l1: 3.6141
[910]	train's l1: 2.02187	test's l1: 3.61305
[920]	train's l1: 2.02128	test's l1: 3.61296
[930]	train's l1: 2.02044	test's l1: 3.6129
[940]	train's l1: 2.00974	test's l1: 3.61014
[950]	train's l1: 2.00913	test's l1: 3.60977
[960]	train's l1: 2.00827	test's l1: 3.60987
[970]	train's l1: 2.00751	test's l1: 3.60987
[980]	train's l1: 1.99531	test's l1: 3.60764
[990]	train's l1: 1.98482	test's l1: 3.60407
[1000]	train's l1: 1.98421	test's l1: 3.60409
Did not meet early stopping. Best iteration is:
[996]	train's l1: 1.98442	test's l1: 3.604
